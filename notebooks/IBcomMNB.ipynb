{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqGMZ8QHFyvg"
   },
   "source": [
    "## Biliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7795,
     "status": "ok",
     "timestamp": 1737511602297,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "V1V8hPS3F5Td",
    "outputId": "2d808c4f-d625-472a-c9c8-217380de6cd2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import json\n",
    "# import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from collections import defaultdict\n",
    "import scipy.special as sc\n",
    "\n",
    "import warnings\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, CategoricalNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# TensorFlow/Keras imports\n",
    "# import tensorflow as tf\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, GRU,SimpleRNN, Dense, Activation, Dropout, Embedding, BatchNormalization\n",
    "# from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "# from keras.layers import TextVectorization\n",
    "#from keras.preprocessing import sequence, text as txt\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25859,
     "status": "ok",
     "timestamp": 1737511628151,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "jSHzu9HDgf_R",
    "outputId": "2f0250f4-9716-4877-debe-3b8c44874371"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_Ws0h7OiF-e"
   },
   "source": [
    "## Importe de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 1146,
     "status": "ok",
     "timestamp": 1737513624773,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "km_wxLRiiCTH"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/MyDrive/Mestrado_códigos/SpamCode/emailNPL/notebooks/corpusFakeRecogna/ohsumed-clean-disease.csv\"\n",
    "\n",
    "\n",
    "data = pd.read_csv(path).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 577,
     "status": "ok",
     "timestamp": 1737513631793,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "g7D362Lhp4OT"
   },
   "outputs": [],
   "source": [
    "text_col = \"text\"\n",
    "label_col = \"Category\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 837
    },
    "executionInfo": {
     "elapsed": 761,
     "status": "ok",
     "timestamp": 1737513628323,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "jHeTuw-BpsHm",
    "outputId": "a2683c8a-6a9e-45cf-f385-7e380c2e5949"
   },
   "outputs": [],
   "source": [
    "data[label_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "executionInfo": {
     "elapsed": 568,
     "status": "ok",
     "timestamp": 1737513634880,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "Xoi5WdBl7xVx",
    "outputId": "b2eccd2b-1105-443d-846b-661d1c608c4e"
   },
   "outputs": [],
   "source": [
    "categories_to_remove = [\n",
    "    'Neoplasms',\n",
    "    'Digestive System Diseases',\n",
    "    'Pathological Conditions', 'Respiratory Tract Diseases',\n",
    "    'Urologic and Male Genital Diseases', 'Disorders of Environmental Origin',\n",
    "    'Musculoskeletal Diseases', 'Immunologic Diseases',\n",
    "    'Nutritional and Metabolic Diseases', 'Virus Diseases',\n",
    "    'Female Genital Diseases and Pregnancy Complications',\n",
    "    'Skin and Connective Tissue Diseases', 'Eye Diseases',\n",
    "    'Hemic and Lymphatic Diseases', 'Neonatal Diseases and Abnormalities',\n",
    "    'Otorhinolaryngologic Diseases', 'Parasitic Diseases',\n",
    "    'Stomatognathic Diseases', 'Endocrine Diseases', 'Animal Diseases'\n",
    "]\n",
    "\n",
    "\n",
    "data_filter = data[~data['Category'].isin(categories_to_remove)]\n",
    "data_filter[label_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1737513638105,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "WDirF6kSqIjc",
    "outputId": "b20ac6d0-838c-4efb-d7a3-3eef4f5ed332"
   },
   "outputs": [],
   "source": [
    "X = data_filter[text_col].values.reshape(-1,1)  # Features\n",
    "y = data_filter[label_col]  # Target\n",
    "\n",
    "sampling_strategy_under = {\n",
    "    \"Cardiovascular Diseases\": 4600,\n",
    "    \"Nervous System Diseases\": 2300,\n",
    "    \"Bacterial Infections and Mycoses\": 2300,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "undersampler = RandomUnderSampler(sampling_strategy=sampling_strategy_under, random_state=42) #reduzir número de amostras\n",
    "X_under, y_under = undersampler.fit_resample(X, y)\n",
    "\n",
    "\n",
    "data_df = pd.DataFrame({\n",
    "    text_col: X_under.flatten(),  # converter de matriz 2D para 1D\n",
    "    label_col: y_under\n",
    "})\n",
    "\n",
    "print(\"Distribuição final das categorias:\")\n",
    "print(data_df[label_col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1737513640218,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "z80Zoq7Qm-24"
   },
   "outputs": [],
   "source": [
    "text = 'text'\n",
    "target = 'Category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ry8AxHvqabW"
   },
   "outputs": [],
   "source": [
    "data_df = data_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTooar5PQMTL"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/MyDrive/Mestrado_códigos/SpamCode/emailNPL/notebooks/corpusFakeRecogna/categories-ohsumed-texts_filtrado.csv\"\n",
    "\n",
    "\n",
    "data = pd.read_csv(path).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogOiZj3rGADi"
   },
   "source": [
    "## contagem de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZeclbynQwFX"
   },
   "outputs": [],
   "source": [
    "# path = \"/content/drive/MyDrive/Mestrado_códigos/SpamCode/emailNPL/notebooks/corpusFakeRecogna/categories-ohsumed-texts.csv\"\n",
    "\n",
    "# data = pd.read_csv(path).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# # Função para selecionar a primeira categoria\n",
    "# def get_first_category(category):\n",
    "#     return category.split(\",\")[0].strip()\n",
    "\n",
    "# # Aplicando a função para manter apenas a primeira categoria\n",
    "# data[\"Category\"] = data[\"Category\"].apply(get_first_category)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDOGjhNjSjQQ"
   },
   "outputs": [],
   "source": [
    "# text = 'text'\n",
    "# target = 'Category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1737501903635,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "5H7-krm1K9-U",
    "outputId": "176ef832-eafb-417a-b5e1-739e1dc29e6d"
   },
   "outputs": [],
   "source": [
    "# categories_to_remove = [\n",
    "#     'Neoplasms',\n",
    "#     'Digestive System Diseases',\n",
    "#     'Pathological Conditions', 'Respiratory Tract Diseases',\n",
    "#     'Urologic and Male Genital Diseases', 'Disorders of Environmental Origin',\n",
    "#     'Musculoskeletal Diseases', 'Immunologic Diseases',\n",
    "#     'Nutritional and Metabolic Diseases', 'Virus Diseases',\n",
    "#     'Female Genital Diseases and Pregnancy Complications',\n",
    "#     'Skin and Connective Tissue Diseases', 'Eye Diseases',\n",
    "#     'Hemic and Lymphatic Diseases', 'Neonatal Diseases and Abnormalities',\n",
    "#     'Otorhinolaryngologic Diseases', 'Parasitic Diseases',\n",
    "#     'Stomatognathic Diseases', 'Endocrine Diseases', 'Animal Diseases'\n",
    "# ]\n",
    "\n",
    "\n",
    "# data_filter = data[~data['Category'].isin(categories_to_remove)]\n",
    "# data_filter[target].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1737501907453,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "eK-u13iQQMaq",
    "outputId": "b3ab54c2-1bbe-4e3f-8400-dbee20284c1d"
   },
   "outputs": [],
   "source": [
    "# X = data_filter[text].values.reshape(-1,1)  # Features\n",
    "# y = data_filter[target]  # Target\n",
    "\n",
    "# sampling_strategy_under = {\n",
    "#     \"Cardiovascular Diseases\": 4600,\n",
    "#     \"Nervous System Diseases\": 2300,\n",
    "#     \"Bacterial Infections and Mycoses\": 2300,\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# undersampler = RandomUnderSampler(sampling_strategy=sampling_strategy_under, random_state=42) #reduzir número de amostras\n",
    "# X_under, y_under = undersampler.fit_resample(X, y)\n",
    "\n",
    "\n",
    "# data_df = pd.DataFrame({\n",
    "#     text: X_under.flatten(),  # converter de matriz 2D para 1D\n",
    "#     target: y_under\n",
    "# })\n",
    "\n",
    "# print(\"Distribuição final das categorias:\")\n",
    "# print(data_df[target].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 1044,
     "status": "ok",
     "timestamp": 1737501963402,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "zjL3g50YL90h",
    "outputId": "be1550eb-8867-4ab7-d9dd-16b6e2ab285d"
   },
   "outputs": [],
   "source": [
    "# data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1737501918434,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "PC7qOgq3quAU",
    "outputId": "780700b8-71d3-4ed0-d9d5-133555f8eceb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# word_number = [len(doc.split()) for doc in data_df[text]]\n",
    "\n",
    "# result_word = sum(word_number)\n",
    "\n",
    "# print(f\"total de palavras: {result_word}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqYHyiPIoZDV"
   },
   "source": [
    "## K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1737513180079,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "ecX9epVXojDI"
   },
   "outputs": [],
   "source": [
    "K = 5\n",
    "data_df['fold'] = [i % K for i in range(len(data_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1737477087823,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "LtOPKjXiroW3",
    "outputId": "b75a51e1-2c8b-4ddb-bb3f-a69bd34fd69b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Modificando as labels\n",
    "data_df['Category'] = data_df['Category'].apply(lambda x: 'Cardiovascular Diseases' if x == 'Cardiovascular Diseases' else 'NoCardiovascularDisease')\n",
    "\n",
    "data_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1737477121447,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "m7IACiroscGH",
    "outputId": "7649cf95-3f33-4804-aed2-a0beacd3760f"
   },
   "outputs": [],
   "source": [
    "data_df['Category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxINxMlMo5lL"
   },
   "source": [
    "## Create Words Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1737511359397,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "sGECRJqgvoXL"
   },
   "outputs": [],
   "source": [
    "## novo create words dict\n",
    "\n",
    "# 1. Função para criar dicionário de palavras agrupadas pelo Intersect Bayes\n",
    "def create_group_dict(train, text, target, solution_func):\n",
    "    group_dict = defaultdict(set)\n",
    "    for i, entry in enumerate(train[text]):\n",
    "        pairs = solution_func(entry)  # Obtém grupos usando Intersect Bayes\n",
    "        for group in pairs:\n",
    "            group_id = f\"{group['a']}_{group['b']}\"  # ID único do grupo\n",
    "            group_dict[group_id].add(i)\n",
    "    return group_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1737511359398,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "wcj8Rgmgo8Q-"
   },
   "outputs": [],
   "source": [
    "# count words without max_feature. 0  = fake news and 1 = real\n",
    "def create_words_dict(train, text, target):\n",
    "    words_dict_0, words_dict_1, words_dict = dict(), dict(), dict()\n",
    "\n",
    "    for i, (entry, yi) in enumerate(zip(train[text], train[target])):\n",
    "        for word in set(entry.split(\" \")):\n",
    "            if word in words_dict:\n",
    "                words_dict[word].add(i)\n",
    "            else:\n",
    "                words_dict[word] = {i}\n",
    "            if yi == 'Cardiovascular Diseases':\n",
    "                if word in words_dict_0:\n",
    "                    words_dict_0[word].add(i)\n",
    "                else:\n",
    "                    words_dict_0[word] = {i}\n",
    "            else:\n",
    "                if word in words_dict_1:\n",
    "                    words_dict_1[word].add(i)\n",
    "                else:\n",
    "                    words_dict_1[word] = {i}\n",
    "\n",
    "    print(f\"words_dict_0: {len(words_dict_0)} words_dict_1:{len(words_dict_1)}\")\n",
    "\n",
    "    return words_dict_1, words_dict_0, words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1265,
     "status": "ok",
     "timestamp": 1737477664090,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "uL5N8IWmpMy9",
    "outputId": "9492115c-f0b7-41ae-9e78-6bab9989bfd4"
   },
   "outputs": [],
   "source": [
    "words_dict_1, words_dict_0, words_dict = create_words_dict(data_df, text, target)\n",
    "vocabulary_size = len(words_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0ZUvOmKr0rs"
   },
   "source": [
    "## IntersectBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PecLCJOJr5Wi"
   },
   "outputs": [],
   "source": [
    "# Função para processar o particionamento da versão exata\n",
    "def solution_clusters(js):\n",
    "    remaining = set(entry.split(' '))\n",
    "    solution = []\n",
    "    for cluster in js['Clusters']:\n",
    "        if len(cluster['Intersection']) > 0:\n",
    "            a = entry_dict[cluster['Elements'][0]]\n",
    "            b = entry_dict[cluster['Elements'][1]]\n",
    "            intersection_0 = intersection_1 = set()\n",
    "            if a in words_dict_0 and b in words_dict_0:\n",
    "                intersection_0 = words_dict_0[a] & words_dict_0[b]\n",
    "            if a in words_dict_1 and b in words_dict_1:\n",
    "                intersection_1 = words_dict_1[a] & words_dict_1[b]\n",
    "            solution.append({\n",
    "                'a': a,\n",
    "                'b': b,\n",
    "                'int': len(cluster['Intersection']),\n",
    "                'int_0': len(intersection_0),\n",
    "                'int_1': len(intersection_1)\n",
    "            })\n",
    "            for element in cluster['Elements']:\n",
    "                remaining.remove(entry_dict[element])\n",
    "            if len(remaining) <= 1:\n",
    "                break\n",
    "\n",
    "    for word in remaining:\n",
    "        if word in words_dict:\n",
    "            solution.append({\n",
    "                    'a': word,\n",
    "                    'b': word,\n",
    "                    'int': len(words_dict[word]),\n",
    "                    'int_0': len(words_dict_0[word]) if word in words_dict_0 else 0,\n",
    "                    'int_1': len(words_dict_1[word]) if word in words_dict_1 else 0\n",
    "                })\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRKLpKGUUwzd"
   },
   "outputs": [],
   "source": [
    "# Classe que cria os candidatos a serem grupos\n",
    "class Pairs:\n",
    "    def __init__(self, words_dict, words_dict_0, words_dict_1):\n",
    "        self.pairs_matrix = dict()\n",
    "        self.words_dict = words_dict\n",
    "        self.words_dict_1 = words_dict_1\n",
    "        self.words_dict_0 = words_dict_0\n",
    "\n",
    "    def get_pairs(self, entry):\n",
    "        pairs = []\n",
    "        words = list(set(entry.split(' ')))\n",
    "        for j in range(len(words)):\n",
    "            for k in range(j+1, len(words)):\n",
    "                if words[j] not in self.words_dict or words[k] not in self.words_dict:\n",
    "                    continue\n",
    "                if words[j] in self.pairs_matrix and words[k] in self.pairs_matrix[words[j]]:\n",
    "                    if self.pairs_matrix[words[j]][words[k]][0] > 0:\n",
    "                        pairs.append({\n",
    "                            'a': words[j],\n",
    "                            'b': words[k],\n",
    "                            'int': self.pairs_matrix[words[j]][words[k]][0],\n",
    "                            'int_0': self.pairs_matrix[words[j]][words[k]][1],\n",
    "                            'int_1': self.pairs_matrix[words[j]][words[k]][2]\n",
    "                        })\n",
    "                    continue\n",
    "\n",
    "                intersection = self.words_dict[words[j]] & self.words_dict[words[k]]\n",
    "                intersection_0, intersection_1 = dict(), dict()\n",
    "                if words[j] in self.words_dict_0 and words[k] in self.words_dict_0:\n",
    "                    intersection_0 = words_dict_0[words[j]] & self.words_dict_0[words[k]]\n",
    "                if words[j] in self.words_dict_1 and words[k] in self.words_dict_1:\n",
    "                    intersection_1 = self.words_dict_1[words[j]] & self.words_dict_1[words[k]]\n",
    "\n",
    "                vec = [len(intersection), len(intersection_0), len(intersection_1)]\n",
    "                if words[j] not in self.pairs_matrix:\n",
    "                    self.pairs_matrix[words[j]] = {words[k]: vec}\n",
    "                else:\n",
    "                    self.pairs_matrix[words[j]][words[k]] = vec\n",
    "                if words[k] not in self.pairs_matrix:\n",
    "                    self.pairs_matrix[words[k]] = {words[j]: vec}\n",
    "                else:\n",
    "                    self.pairs_matrix[words[k]][words[j]] = vec\n",
    "\n",
    "                if len(intersection) > 0:\n",
    "                    pairs.append({\n",
    "                        'a': words[j],\n",
    "                        'b': words[k],\n",
    "                        'int': self.pairs_matrix[words[j]][words[k]][0],\n",
    "                        'int_0': self.pairs_matrix[words[j]][words[k]][1],\n",
    "                        'int_1': self.pairs_matrix[words[j]][words[k]][2]\n",
    "                    })\n",
    "\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_2LSn5SUydG"
   },
   "outputs": [],
   "source": [
    "# Heuristica gulosa\n",
    "# pairs = candidatos, supoe-se que está ordenado pelo tamanho da interseção\n",
    "# entry = texto do email\n",
    "def get_solution(pairs, entry):\n",
    "    remaining = set(entry.split(' '))\n",
    "    solution = []\n",
    "    # Se houver grupos\n",
    "    if len(pairs) > 0:\n",
    "        # Para cada grupo\n",
    "        for row in pairs.itertuples():\n",
    "            # Se o grupo pode ser adicionado no particionamento, o adicone\n",
    "            if row.a in remaining and row.b in remaining:\n",
    "                solution.append({\n",
    "                    'a': row.a,\n",
    "                    'b': row.b,\n",
    "                    'int': row.int,\n",
    "                    'int_0': row.int_0,\n",
    "                    'int_1': row.int_1\n",
    "                })\n",
    "                remaining.remove(row.a)\n",
    "                remaining.remove(row.b)\n",
    "                if len(remaining) <= 1:\n",
    "                    break\n",
    "\n",
    "    # Caso existam palavras fora do particionamento, criar grupos unitários para elas.\n",
    "    # Tanto faz criar um único ou vários, não muda a função objetivo\n",
    "    for word in remaining:\n",
    "        if word in words_dict:\n",
    "            solution.append({\n",
    "                    'a': word,\n",
    "                    'b': word,\n",
    "                    'int': len(words_dict[word]),\n",
    "                    'int_0': len(words_dict_0[word]) if word in words_dict_0 else 0,\n",
    "                    'int_1': len(words_dict_1[word]) if word in words_dict_1 else 0\n",
    "                })\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeCjnSIJVPPC"
   },
   "source": [
    "### Predição utilizando particionamentos $P^{fakenews}$ e $P^{real}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1737477733659,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "R8Pl9975VL0_",
    "outputId": "5883a193-89c1-47e9-c134-07f8e2eb4f84"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 0.001\n",
    "\n",
    "f1_2, auc_2, acc_2, recall_2, precision_2 = (\n",
    "    np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5)\n",
    ")\n",
    "\n",
    "for fold in range(5):\n",
    "    break\n",
    "    print(f\"Fold {fold}/5\")\n",
    "\n",
    "    train = data_df.loc[data_df['fold'] != fold, :]\n",
    "    test = data_df.loc[data['fold'] == fold, :]\n",
    "\n",
    "    words_dict_1, words_dict_0, words_dict = create_words_dict(train, text, target)\n",
    "    obj_pairs_0 = Pairs(words_dict, words_dict_0, {})\n",
    "    obj_pairs_1 = Pairs(words_dict, {}, words_dict_1)\n",
    "\n",
    "    prioris = train[target].value_counts(normalize=True)\n",
    "    counts = train[target].value_counts(normalize=False)\n",
    "\n",
    "    xtest_prob_0, xtest_prob_1 = np.zeros(len(test)), np.zeros(len(test))\n",
    "    solutions = np.zeros(len(test))\n",
    "    for i, entry in enumerate(test[text]):\n",
    "        print(f\"{i}/{len(test)}\")\n",
    "        try:\n",
    "            pairs_0 = obj_pairs_0.get_pairs(entry)\n",
    "            pairs_1 = obj_pairs_1.get_pairs(entry)\n",
    "\n",
    "            pairs_0 = pd.DataFrame(pairs_0)\n",
    "            pairs_1 = pd.DataFrame(pairs_1)\n",
    "            if len(pairs_0) > 0:\n",
    "                pairs_0 = pairs_0.sort_values(by='int', ascending=False)\n",
    "            if len(pairs_1) > 0:\n",
    "                pairs_1 = pairs_1.sort_values(by='int', ascending=False)\n",
    "\n",
    "            solution_0 = pd.DataFrame(get_solution(pairs_0, entry)).sort_values(by='int', ascending=False)\n",
    "            solution_1 = pd.DataFrame(get_solution(pairs_1, entry)).sort_values(by='int', ascending=False)\n",
    "\n",
    "            priori_words_0 = priori_words_1 = 1\n",
    "            solutions[i] = len(solution_0) + len(solution_1)\n",
    "            for row in solution_0.itertuples():\n",
    "                    priori_words_0 *= (k + row.int_0) / (2*k + row.int)\n",
    "            for row in solution_1.itertuples():\n",
    "                    priori_words_1 *= (k + row.int_1) / (2*k + row.int)\n",
    "\n",
    "            xtest_prob_0[i] = (priori_words_0 * prioris['Cardiovascular Diseases']) / (priori_words_0 * prioris['Cardiovascular Diseases'] +\n",
    "                                                                priori_words_1 * prioris['NoCardiovascularDisease'])\n",
    "            xtest_prob_1[i] = (priori_words_1 * prioris['NoCardiovascularDisease']) / (priori_words_0 * prioris['Cardiovascular Diseases'] +\n",
    "                                                                 priori_words_1 * prioris['NoCardiovascularDisease'])\n",
    "            #print(f\"I = {solution['int'].sum()} I_0 = {solution['int_0'].sum()} I_1 = {solution['int_1'].sum()}\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            #print(f\"Can't label {i}, '{entry}' words not present in train\")\n",
    "\n",
    "     # Evaluate\n",
    "    pred = xtest_prob_0 < xtest_prob_1\n",
    "    pred_proba = np.nan_to_num(xtest_prob_1, nan=0.5)\n",
    "    f1_2[fold] = f1_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred)\n",
    "    auc_2[fold] = roc_auc_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred_proba)\n",
    "    acc_2[fold] = accuracy_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred)\n",
    "    recall_2[fold] = recall_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred)\n",
    "    precision_2[fold] = precision_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1737477735820,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "rKdUKkkAVMBx",
    "outputId": "c80130e4-b2f9-4acc-ad01-3613a675bd79"
   },
   "outputs": [],
   "source": [
    "print(f\"Precision: {precision_2.mean():.4f} +/- {precision_2.std():.4f}\")\n",
    "print(f\"Recall: {recall_2.mean():.4f} +/- {recall_2.std():.4f}\")\n",
    "print(f\"Accuracy: {acc_2.mean():.4f} +/- {acc_2.std():.4f}\")\n",
    "print(f\"F1: {f1_2.mean():.4f} +/- {f1_2.std():.4f}\")\n",
    "print(f\"ROC AUC: {auc_2.mean():.4f} +/- {auc_2.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPvvIUUe-Qxm"
   },
   "source": [
    "# Intersect bayes usando a probabilidade do MNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4324784,
     "status": "ok",
     "timestamp": 1737408366416,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "xsf9Odvq-QSc",
    "outputId": "f2ebe887-e1bf-4f57-a4bc-279b910ccbd5"
   },
   "outputs": [],
   "source": [
    "#intersect bayes\n",
    "%%time\n",
    "k = 0.001\n",
    "\n",
    "f1, auc, acc, recall, precision = (\n",
    "    np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5)\n",
    ")\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"Fold {fold}/5\")\n",
    "\n",
    "    train = data.loc[data['fold'] != fold, :]\n",
    "    test = data.loc[data['fold'] == fold, :]\n",
    "\n",
    "    words_dict_1, words_dict_0, words_dict = create_words_dict(train, text, target)\n",
    "    obj_pairs = Pairs(words_dict, words_dict_0, words_dict_1)\n",
    "\n",
    "    prioris = train[target].value_counts(normalize=True)\n",
    "    print(prioris)\n",
    "    # counts = train[target].value_counts(normalize=False)\n",
    "\n",
    "    xtest_prob_0, xtest_prob_1 = np.zeros(len(test)), np.zeros(len(test))\n",
    "    solutions = np.zeros(len(test))\n",
    "    for i, entry in enumerate(test[text]):\n",
    "        print(f\"{i}/{len(test)}\")\n",
    "        try:\n",
    "            pairs = pd.DataFrame(obj_pairs.get_pairs(entry))\n",
    "            if len(pairs) > 0:\n",
    "                pairs = pairs.sort_values(by='int', ascending=False)\n",
    "            solution = pd.DataFrame(get_solution(pairs, entry)).sort_values(by='int', ascending=False)\n",
    "            priori_words_0 = prioris['real']\n",
    "            priori_words_1 = prioris['fakenews']\n",
    "            solutions[i] = len(solution)\n",
    "            for row in solution.itertuples():\n",
    "              priori_words_0 *= (k + row.int_0) / (k * len(words_dict_0) + sum(len(v) for v in words_dict_0.values()))\n",
    "              priori_words_1 *= (k + row.int_1) / (k * len(words_dict_1) + sum(len(v) for v in words_dict_1.values()))\n",
    "\n",
    "\n",
    "              xtest_prob_0[i] = (priori_words_0 * prioris['real']) / (priori_words_0 * prioris['real'] +\n",
    "                                                                priori_words_1 * prioris['fakenews'])\n",
    "              xtest_prob_1[i] = (priori_words_1 * prioris['fakenews']) / (priori_words_0 * prioris['real'] +\n",
    "                                                                 priori_words_1 * prioris['fakenews'])\n",
    "            #print(f\"I = {solution['int'].sum()} I_0 = {solution['int_0'].sum()} I_1 = {solution['int_1'].sum()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Can't label {i}, '{entry}' words not present in train\")\n",
    "\n",
    "     # Evaluate\n",
    "    pred = xtest_prob_0 < xtest_prob_1\n",
    "    pred_proba = np.nan_to_num(xtest_prob_1, nan=0.5)\n",
    "    f1[fold] = f1_score(test[target].apply(lambda x: True if x == 'fakenews' else False),\n",
    "                                pred)\n",
    "    auc[fold] = roc_auc_score(test[target].apply(lambda x: True if x == 'fakenews' else False),\n",
    "                                pred_proba)\n",
    "    acc[fold] = accuracy_score(test[target].apply(lambda x: True if x == 'fakenews' else False),\n",
    "                                pred)\n",
    "    recall[fold] = recall_score(test[target].apply(lambda x: True if x == 'fakenews' else False),\n",
    "                                pred)\n",
    "    precision[fold] = precision_score(test[target].apply(lambda x: True if x == 'fakenews' else False),\n",
    "                                pred)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1737408367036,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "XtimW8JdFeiJ",
    "outputId": "599c99a1-90b4-4101-9a65-7c36b0e2f3f8"
   },
   "outputs": [],
   "source": [
    "print(f\"Recall: {recall.mean():.4f} +/- {recall.std():.4f}\")\n",
    "print(f\"Precision: {precision.mean():.4f} +/- {precision.std():.4f}\")\n",
    "print(f\"Accuracy: {acc.mean():.4f} +/- {acc.std():.4f}\")\n",
    "print(f\"F1: {f1.mean():.4f} +/- {f1.std():.4f}\")\n",
    "print(f\"ROC AUC: {auc.mean():.4f} +/- {auc.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKlzXMD1uMTv"
   },
   "source": [
    "# Predição utilizando particionamento único - IB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 266577,
     "status": "ok",
     "timestamp": 1737478035133,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "n5N7A7WjuTSZ",
    "outputId": "e67266d3-4c54-4ca1-bf11-0ba49e56d13b"
   },
   "outputs": [],
   "source": [
    "#intersect bayes\n",
    "%%time\n",
    "k = 0.001\n",
    "\n",
    "f1, auc, acc, recall, precision = (\n",
    "    np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5)\n",
    ")\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"Fold {fold}/5\")\n",
    "\n",
    "    train = data_df.loc[data_df['fold'] != fold, :]\n",
    "    test = data_df.loc[data_df['fold'] == fold, :]\n",
    "\n",
    "    words_dict_1, words_dict_0, words_dict = create_words_dict(train, text, target)\n",
    "    obj_pairs = Pairs(words_dict, words_dict_0, words_dict_1)\n",
    "\n",
    "    prioris = train[target].value_counts(normalize=True)\n",
    "    counts = train[target].value_counts(normalize=False)\n",
    "\n",
    "    xtest_prob_0, xtest_prob_1 = np.zeros(len(test)), np.zeros(len(test))\n",
    "    solutions = np.zeros(len(test))\n",
    "    for i, entry in enumerate(test[text]):\n",
    "        print(f\"{i}/{len(test)}\")\n",
    "        try:\n",
    "            pairs = pd.DataFrame(obj_pairs.get_pairs(entry))\n",
    "            if len(pairs) > 0:\n",
    "                pairs = pairs.sort_values(by='int', ascending=False)\n",
    "            solution = pd.DataFrame(get_solution(pairs, entry)).sort_values(by='int', ascending=False)\n",
    "            priori_words_0 = priori_words_1 = 1\n",
    "            solutions[i] = len(solution)\n",
    "            for row in solution.itertuples():\n",
    "                    priori_words_0 *= (k + row.int_0) / (2*k + row.int)\n",
    "                    priori_words_1 *= (k + row.int_1) / (2*k + row.int)\n",
    "\n",
    "            xtest_prob_0[i] = (priori_words_0 * prioris['Cardiovascular Diseases']) / (priori_words_0 * prioris['Cardiovascular Diseases'] +\n",
    "                                                                priori_words_1 * prioris['NoCardiovascularDisease'])\n",
    "            xtest_prob_1[i] = (priori_words_1 * prioris['NoCardiovascularDisease']) / (priori_words_0 * prioris['Cardiovascular Diseases'] +\n",
    "                                                                 priori_words_1 * prioris['NoCardiovascularDisease'])\n",
    "            #print(f\"I = {solution['int'].sum()} I_0 = {solution['int_0'].sum()} I_1 = {solution['int_1'].sum()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Can't label {i}, '{entry}' words not present in train\")\n",
    "\n",
    "     # Evaluate\n",
    "    pred = xtest_prob_0 < xtest_prob_1\n",
    "    pred_proba = np.nan_to_num(xtest_prob_1, nan=0.5)\n",
    "    f1[fold] = f1_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred)\n",
    "    auc[fold] = roc_auc_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred_proba)\n",
    "    acc[fold] = accuracy_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred)\n",
    "    recall[fold] = recall_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred)\n",
    "    precision[fold] = precision_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737478075261,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "kR4iCxS8uazC",
    "outputId": "4d8ae3fb-b3cd-4da5-ae06-53d0f01c945b"
   },
   "outputs": [],
   "source": [
    "print(f\"Recall: {recall.mean():.4f} +/- {recall.std():.4f}\")\n",
    "print(f\"Precision: {precision.mean():.4f} +/- {precision.std():.4f}\")\n",
    "print(f\"Accuracy: {acc.mean():.4f} +/- {acc.std():.4f}\")\n",
    "print(f\"F1: {f1.mean():.4f} +/- {f1.std():.4f}\")\n",
    "print(f\"ROC AUC: {auc.mean():.4f} +/- {auc.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21553,
     "status": "ok",
     "timestamp": 1737478884011,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "cplJcsWOuhiv",
    "outputId": "14c96bea-bab5-4da8-d0e1-97cedd4c799b"
   },
   "outputs": [],
   "source": [
    "#naive bayes\n",
    "%%time\n",
    "k = 0.001\n",
    "\n",
    "f1_pure, auc_pure, acc_pure, recall_pure, precision_pure = (\n",
    "    np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5)\n",
    ")\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"Fold {fold}/5\")\n",
    "\n",
    "    train = data_df.loc[data_df['fold'] != fold, :]\n",
    "    test = data_df.loc[data_df['fold'] == fold, :]\n",
    "\n",
    "    words_dict_1, words_dict_0, words_dict = create_words_dict(train, text, target)\n",
    "\n",
    "    prioris = train[target].value_counts(normalize=True)\n",
    "    counts = train[target].value_counts(normalize=False)\n",
    "\n",
    "    xtest_prob_0_pure, xtest_prob_1_pure = np.zeros(len(test)), np.zeros(len(test))\n",
    "\n",
    "    for i, entry in enumerate(test[text]):\n",
    "        priori_words_0 = priori_words_1 = 1\n",
    "        for word in set(entry.split(\" \")):\n",
    "            if word in words_dict_0:\n",
    "                priori_words_0 *= (k + len(words_dict_0[word])) / (2*k + counts[0])\n",
    "            else:\n",
    "                priori_words_0 *= k / 2*k\n",
    "            if word in words_dict_1:\n",
    "                priori_words_1 *= (k + len(words_dict_1[word])) / (2*k + counts[1])\n",
    "            else:\n",
    "                priori_words_1 *= k / 2*k\n",
    "\n",
    "        xtest_prob_0_pure[i] = (priori_words_0 * prioris['Cardiovascular Diseases']) / (priori_words_0 * prioris['Cardiovascular Diseases'] +\n",
    "                                                               priori_words_1 * prioris['NoCardiovascularDisease'])\n",
    "        xtest_prob_1_pure[i] = (priori_words_1 * prioris['NoCardiovascularDisease']) / (priori_words_0 * prioris['Cardiovascular Diseases'] +\n",
    "                                                                priori_words_1 * prioris['NoCardiovascularDisease'])\n",
    "    # Evaluate\n",
    "    pred_pure = xtest_prob_0_pure < xtest_prob_1_pure\n",
    "    pred_proba_pure = np.nan_to_num(xtest_prob_1_pure, nan=0.5)\n",
    "    f1_pure[fold] = f1_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred_pure)\n",
    "    auc_pure[fold] = roc_auc_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred_proba_pure)\n",
    "    acc_pure[fold] = accuracy_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred_pure)\n",
    "    recall_pure[fold] = recall_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred_pure)\n",
    "    precision_pure[fold] = precision_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred_pure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1737478888600,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "AtRz58S9uimA",
    "outputId": "06c2734b-8826-49cc-ebbc-31063983da7f"
   },
   "outputs": [],
   "source": [
    "print(f\"Recall: {recall_pure.mean():.4f} +/- {recall_pure.std():.4f}\")\n",
    "print(f\"Precision: {precision_pure.mean():.4f} +/- {precision_pure.std():.4f}\")\n",
    "print(f\"Accuracy: {acc_pure.mean():.4f} +/- {acc_pure.std():.4f}\")\n",
    "print(f\"F1: {f1_pure.mean():.4f} +/- {f1_pure.std():.4f}\")\n",
    "print(f\"ROC AUC: {auc_pure.mean():.4f} +/- {auc_pure.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGc7iZ1Yu0OZ"
   },
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXouO07cuzpg"
   },
   "outputs": [],
   "source": [
    "def CountOrTfidf_vec(train,test,opt):\n",
    "#does the transformation for given type of vectorizer\n",
    "    if(opt == \"tfidf\"):\n",
    "        transformer = TfidfVectorizer(min_df=5, max_features=30000)\n",
    "    elif(opt == \"count\"):\n",
    "        transformer = CountVectorizer(min_df=5, max_features=30000)\n",
    "    train_CorTf = transformer.fit_transform(train)\n",
    "    test_CorTf = transformer.transform(test)\n",
    "    return train_CorTf,test_CorTf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28077,
     "status": "ok",
     "timestamp": 1737478951401,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "5ljbtU3Nu9Lr",
    "outputId": "13725fbd-bc23-448b-8443-cd10c5364151"
   },
   "outputs": [],
   "source": [
    "#multinomial Naive Bayes\n",
    "f1_mnb, auc_mnb, acc_mnb, recall_mnb, precision_mnb = (\n",
    "    np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5)\n",
    ")\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"Fold {fold}/5\")\n",
    "\n",
    "    train = data_df.loc[data_df['fold'] != fold, :]\n",
    "    test = data_df.loc[data_df['fold'] == fold, :]\n",
    "\n",
    "    x_trainCorT, x_testCorT = CountOrTfidf_vec(train[text], test[text], \"count\")\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(x_trainCorT.toarray(), train[target].apply(lambda x: 1 if x == 'NoCardiovascularDisease' else 0))\n",
    "\n",
    "    pred_mnb = nb.predict(x_testCorT.toarray())\n",
    "    pred_proba_mnb = nb.predict_proba(x_testCorT.toarray())[:, 1]\n",
    "\n",
    "    f1_mnb[fold] = f1_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred_mnb)\n",
    "    auc_mnb[fold] = roc_auc_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred_proba_mnb)\n",
    "    acc_mnb[fold] = accuracy_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred_mnb)\n",
    "    recall_mnb[fold] = recall_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred_mnb)\n",
    "    precision_mnb[fold] = precision_score(test[target].apply(lambda x: True if x == 'NoCardiovascularDisease' else False),\n",
    "                             pred_mnb)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1737479175178,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "rI2ZMwx8u-Fb",
    "outputId": "e41f0e67-5e19-4c34-b240-0c65170f27ce"
   },
   "outputs": [],
   "source": [
    "print(f\"Precision: {precision_mnb.mean():.4f} +/- {precision_mnb.std():.4f}\")\n",
    "print(f\"Recall: {recall_mnb.mean():.4f} +/- {recall_mnb.std():.4f}\")\n",
    "print(f\"Accuracy: {acc_mnb.mean():.4f} +/- {acc_mnb.std():.4f}\")\n",
    "print(f\"F1: {f1_mnb.mean():.4f} +/- {f1_mnb.std():.4f}\")\n",
    "print(f\"ROC AUC: {auc_mnb.mean():.4f} +/- {auc_mnb.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1737479222639,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "rVe-sYkc0hJ2",
    "outputId": "f4ecbc1e-b0c8-436b-a0de-265aab140845"
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Metricas com todas as amostras\": [\"Precision\", \"Recall\", \"Accuracy\", \"F1\", \"ROC AUC\"],\n",
    "\n",
    "    \"Intersect Bayes\": [\n",
    "        f\"{np.mean(precision):.4f} +/- {np.std(precision):.4f}\",\n",
    "        f\"{np.mean(recall):.4f} +/- {np.std(recall):.4f}\",\n",
    "        f\"{np.mean(acc):.4f} +/- {np.std(acc):.4f}\",\n",
    "        f\"{np.mean(f1):.4f} +/- {np.std(f1):.4f}\",\n",
    "        f\"{np.mean(auc):.4f} +/- {np.std(auc):.4f}\",\n",
    "    ],\n",
    "\n",
    "    \"Naive Bayes\": [\n",
    "        f\"{np.mean(precision_pure):.4f} +/- {np.std(precision_pure):.4f}\",\n",
    "        f\"{np.mean(recall_pure):.4f} +/- {np.std(recall_pure):.4f}\",\n",
    "        f\"{np.mean(acc_pure):.4f} +/- {np.std(acc_pure):.4f}\",\n",
    "        f\"{np.mean(f1_pure):.4f} +/- {np.std(f1_pure):.4f}\",\n",
    "        f\"{np.mean(auc_pure):.4f} +/- {np.std(auc_pure):.4f}\",\n",
    "    ],\n",
    "\n",
    "    \"Multinomial Naive Bayes\": [\n",
    "        f\"{np.mean(precision_mnb):.4f} +/- {np.std(precision_mnb):.4f}\",\n",
    "        f\"{np.mean(recall_mnb):.4f} +/- {np.std(recall_mnb):.4f}\",\n",
    "        f\"{np.mean(acc_mnb):.4f} +/- {np.std(acc_mnb):.4f}\",\n",
    "        f\"{np.mean(f1_mnb):.4f} +/- {np.std(f1_mnb):.4f}\",\n",
    "        f\"{np.mean(auc_mnb):.4f} +/- {np.std(auc_mnb):.4f}\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Exibe o DataFrame\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0m2t0j7vDy4"
   },
   "outputs": [],
   "source": [
    "test['pure'] = pred_proba_pure\n",
    "test['intersect'] = pred_proba\n",
    "#test['mnb'] = pred_proba_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVwb5y1UyrSd"
   },
   "outputs": [],
   "source": [
    "def kde(column, target, data, ax=None, title=None, xlabel=\"\", ylabel=\"\"):\n",
    "    if ax is None: fig, ax = plt.subplots(1, 1, figsize=(15, 3))\n",
    "    g = sns.kdeplot(data[column][(data[target] == 'real')], ax=ax, color=\"Red\")\n",
    "    g = sns.kdeplot(data[column][(data[target] == 'fakenews')], ax=g, color=\"Blue\")\n",
    "    g.set_xlabel(xlabel)\n",
    "    g.set_ylabel(ylabel)\n",
    "    if title is not None: g.set_title(title)\n",
    "    #g = g.legend(['Não conluio', 'Conluio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZwBLxh2vF11"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 3), dpi=500)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "kde('pure', 'Classe', test, ax=axs[0], title=\"Naive Bayes\")\n",
    "axs[0].set_ylim([0,25])\n",
    "kde('intersect', 'Classe', test, ax=axs[1], title=\"Intersect Bayes\")\n",
    "axs[1].set_ylim([0,25])\n",
    "#kde('mnb', 'Spam/Ham', test, ax=axs[2])\n",
    "fig.legend([\"$\\overline{fakenews}$\", \"$fakenews$\"])\n",
    "\n",
    "fig.text(0.5, 0.00, 'Probabilidade', ha='center')\n",
    "fig.text(0.08, 0.5, 'Frequência', va='center', rotation='vertical')\n",
    "fig.savefig(\"kde.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkzRNMOFwivN"
   },
   "source": [
    "### testar novo código - ib com mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3409,
     "status": "ok",
     "timestamp": 1737513657408,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "4YwtdXpfwngu",
    "outputId": "71b49396-2994-4b86-ed2f-9544d7ac16a1"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# cria um dicionário de grupos\n",
    "def create_group_dict(train, text, target, solution_func):\n",
    "    group_dict = defaultdict(set)\n",
    "    for i, entry in enumerate(train[text]):\n",
    "        pairs = solution_func(entry)  # interseção de grupos de palavras\n",
    "        for group in pairs:\n",
    "            group_id = f\"{group['a']}_{group['b']}\" # id único\n",
    "            group_dict[group_id].add(i)\n",
    "    return group_dict\n",
    "\n",
    "# substituir palavras pelos ids dos grupos\n",
    "def replace_with_groups(entry, solution_func):\n",
    "    pairs = solution_func(entry)  # pares de palavras\n",
    "    transformed_text = []\n",
    "    for group in pairs:\n",
    "        group_id = f\"{group['a']}_{group['b']}\" # id único\n",
    "        transformed_text.append(group_id)\n",
    "    return ' '.join(transformed_text)\n",
    "\n",
    "\n",
    "# calcular frequência das palavras \n",
    "def calculate_word_frequencies(data, text_column):\n",
    "    word_frequency = defaultdict(int) # dicionário para contar a frequência de palavras\n",
    "    for entry in data[text_column]:\n",
    "        for word in entry.split():\n",
    "            word_frequency[word] += 1\n",
    "    return word_frequency\n",
    "\n",
    "# calcular frequência das palavras \n",
    "class Pairs:\n",
    "    def __init__(self, words_dict, words_dict_0, words_dict_1, word_frequency, min_threshold):\n",
    "        self.pairs_matrix = dict()\n",
    "        self.words_dict = words_dict\n",
    "        self.words_dict_1 = words_dict_1\n",
    "        self.words_dict_0 = words_dict_0\n",
    "        self.word_frequency = word_frequency\n",
    "        self.min_threshold = min_threshold\n",
    "\n",
    "    def get_pairs(self, entry):\n",
    "        pairs = []\n",
    "        words = list(set(entry.split(' ')))\n",
    "        \n",
    "        # filtrar palavras acima da frequencia minima\n",
    "        words = [word for word in words if self.word_frequency[word] > self.min_threshold]\n",
    "\n",
    "        for j in range(len(words)):\n",
    "            for k in range(j+1, len(words)):\n",
    "                if words[j] not in self.words_dict or words[k] not in self.words_dict:\n",
    "                    continue\n",
    "                intersection = self.words_dict[words[j]] & self.words_dict[words[k]]\n",
    "                intersection_0 = self.words_dict_0.get(words[j], set()) & self.words_dict_0.get(words[k], set())\n",
    "                intersection_1 = self.words_dict_1.get(words[j], set()) & self.words_dict_1.get(words[k], set())\n",
    "\n",
    "                if len(intersection) > 0:\n",
    "                    pairs.append({\n",
    "                        'a': words[j],\n",
    "                        'b': words[k],\n",
    "                        'int': len(intersection),\n",
    "                        'int_0': len(intersection_0),\n",
    "                        'int_1': len(intersection_1)\n",
    "                    })\n",
    "        return pairs\n",
    "\n",
    "# heuristica gulosa\n",
    "def get_solution(pairs, entry):\n",
    "    remaining = set(entry.split(' '))\n",
    "    solution = []\n",
    "    if len(pairs) > 0:\n",
    "        for row in pairs:\n",
    "            if row['a'] in remaining and row['b'] in remaining:\n",
    "                solution.append(row)\n",
    "                remaining.remove(row['a'])\n",
    "                remaining.remove(row['b'])\n",
    "                if len(remaining) <= 1:\n",
    "                    break\n",
    "    for word in remaining:\n",
    "        solution.append({'a': word, 'b': word, 'int': 0, 'int_0': 0, 'int_1': 0})\n",
    "    return solution\n",
    "\n",
    "# aplicação do mnb\n",
    "def train_mnb(train, test, text, target, solution_func):\n",
    "   \n",
    "    train[text] = train[text].apply(lambda x: replace_with_groups(x, solution_func))\n",
    "    test[text] = test[text].apply(lambda x: replace_with_groups(x, solution_func))\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train[text])\n",
    "    X_test = vectorizer.transform(test[text])\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, train[target])\n",
    "\n",
    "    predictions = clf.predict(X_test)\n",
    "    probabilities = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(test[target], predictions)\n",
    "    f1 = f1_score(test[target], predictions, average='weighted')\n",
    "    precision = precision_score(test[target], predictions, average='weighted')\n",
    "    recall = recall_score(test[target], predictions, average='weighted')\n",
    "    roc_auc = roc_auc_score(test[target], probabilities)\n",
    "\n",
    "    return accuracy, f1, precision, recall, roc_auc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 80/20 treino/teste\n",
    "    train, test = train_test_split(data_df, test_size=0.2, random_state=42, stratify=data_df[target])\n",
    "\n",
    "    # frequencia das palavras\n",
    "    word_frequency = calculate_word_frequencies(train, text)\n",
    "\n",
    "    # limiar de frequencia minima das palavras\n",
    "    min_threshold = 2\n",
    "\n",
    "    # criação dos dicionários de palavras\n",
    "    words_dict_1, words_dict_0, words_dict = defaultdict(set), defaultdict(set), defaultdict(set)\n",
    "    obj_pairs = Pairs(words_dict, words_dict_0, words_dict_1, word_frequency, min_threshold)\n",
    "\n",
    "    # cria solução\n",
    "    def solution_func(entry):\n",
    "        pairs = obj_pairs.get_pairs(entry)\n",
    "        return get_solution(pairs, entry)\n",
    "\n",
    "\n",
    "    accuracy, f1, precision, recall, roc_auc = train_mnb(train, test, text, target, solution_func)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Roc-Auc: {roc_auc}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPgGvFdOMEqOtTjqzoCGDM0",
   "collapsed_sections": [
    "IPvvIUUe-Qxm"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
