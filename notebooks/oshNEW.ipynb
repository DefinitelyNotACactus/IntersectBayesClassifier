{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1736948369744,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "CpCqt4Wwy2wb",
    "outputId": "5429ca0b-0520-4af0-9aab-64946015cabc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tqdm\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from string import punctuation\n",
    "from scipy.stats import norm\n",
    "import scipy.special as sc\n",
    "\n",
    "import warnings\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from os.path import exists\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, CategoricalNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import label_binarize\n",
    "# TensorFlow/Keras imports\n",
    "# import tensorflow as tf\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, GRU,SimpleRNN, Dense, Activation, Dropout, Embedding, BatchNormalization\n",
    "# from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "# from keras.layers import TextVectorization\n",
    "#from keras.preprocessing import sequence, text as txt\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1736948372541,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "RpCK_aNyy2wg"
   },
   "outputs": [],
   "source": [
    "path = r'C:\\Desktop\\IntersectBayesClassifier\\ohs_data\\ohsumed-clean-disease.csv'\n",
    "data = pd.read_csv(path).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1736948372898,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "3EL6O8dxy2wi",
    "outputId": "55ea09b5-ba7a-4043-c567-5188a2cb4c21"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1736948372898,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "1WDnw_E4y2wj"
   },
   "outputs": [],
   "source": [
    "text_col = \"text\"\n",
    "label_col = \"Category\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 837
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1736948372898,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "ewmx3TCJy2wk",
    "outputId": "e9c1e523-d6a6-449d-ddf2-1fb48ef1accd"
   },
   "outputs": [],
   "source": [
    "data[label_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1736948372898,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "vrj1cqPjy2wl",
    "outputId": "35db1e9d-df8d-4e95-97df-6f7e231ccd13"
   },
   "outputs": [],
   "source": [
    "categories_to_remove = [\n",
    "    'Pathological Conditions', 'Respiratory Tract Diseases',\n",
    "    'Urologic and Male Genital Diseases', 'Disorders of Environmental Origin',\n",
    "    'Musculoskeletal Diseases', 'Immunologic Diseases',\n",
    "    'Nutritional and Metabolic Diseases', 'Virus Diseases',\n",
    "    'Female Genital Diseases and Pregnancy Complications',\n",
    "    'Skin and Connective Tissue Diseases', 'Eye Diseases',\n",
    "    'Hemic and Lymphatic Diseases', 'Neonatal Diseases and Abnormalities',\n",
    "    'Otorhinolaryngologic Diseases', 'Parasitic Diseases',\n",
    "    'Stomatognathic Diseases', 'Endocrine Diseases', 'Animal Diseases'\n",
    "]\n",
    "\n",
    "\n",
    "data_filter = data[~data['Category'].isin(categories_to_remove)]\n",
    "data_filter[label_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1736948372898,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "MziS6RrQy2wl",
    "outputId": "39e014a1-7b06-49d2-95c0-ba5f15545d0f"
   },
   "outputs": [],
   "source": [
    "X = data_filter[text_col].values.reshape(-1,1)  # Features\n",
    "y = data_filter[label_col]  # Target\n",
    "\n",
    "sampling_strategy_under = {\n",
    "    \"Neoplasms\": 2000,\n",
    "    \"Cardiovascular Diseases\": 2000,\n",
    "    \"Nervous System Diseases\": 2000,\n",
    "    \"Bacterial Infections and Mycoses\": 2000,\n",
    "    \"Digestive System Diseases\": 2000\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "undersampler = RandomUnderSampler(sampling_strategy=sampling_strategy_under, random_state=42) #reduzir número de amostras\n",
    "X_under, y_under = undersampler.fit_resample(X, y)\n",
    "\n",
    "\n",
    "data_df = pd.DataFrame({\n",
    "    text_col: X_under.flatten(),  # converter de matriz 2D para 1D\n",
    "    label_col: y_under\n",
    "})\n",
    "\n",
    "print(\"Distribuição final das categorias:\")\n",
    "print(data_df[label_col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1736948372898,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "Dj8mtbX2y2wm"
   },
   "outputs": [],
   "source": [
    "text_col = \"text\"\n",
    "label_col = \"Category\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1736948372898,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "GGdK_4P0y2wn"
   },
   "outputs": [],
   "source": [
    "data_df = data_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Words data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1189,
     "status": "ok",
     "timestamp": 1736948374075,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "bpvNUudFy2wp"
   },
   "outputs": [],
   "source": [
    "word_encoding_dict = dict()\n",
    "label_encoding_dict = dict()\n",
    "\n",
    "presence_word_dict = dict()\n",
    "presence_label_dict = dict()\n",
    "\n",
    "num_words = num_labels = 0\n",
    "for i, (text, label) in enumerate(zip(data_df[text_col], data_df[label_col])):\n",
    "    # Label encoding\n",
    "    if label in label_encoding_dict:\n",
    "        presence_label_dict[label_encoding_dict[label]].add(i)\n",
    "    else:\n",
    "        num_labels += 1\n",
    "        label_encoding_dict[label] = num_labels\n",
    "        presence_label_dict[num_labels] = {i}\n",
    "    # Text encoding\n",
    "    splitted = text.split(\" \")\n",
    "    for word in splitted:\n",
    "        if word in word_encoding_dict:\n",
    "            presence_word_dict[word_encoding_dict[word]].add(i)\n",
    "        else:\n",
    "            num_words += 1\n",
    "            word_encoding_dict[word] = num_words\n",
    "            presence_word_dict[num_words] = {i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1736948374076,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "rAEHkhKvy2wp"
   },
   "outputs": [],
   "source": [
    "# prob_matrix = np.zeros((200, 5))\n",
    "# prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1736948374076,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "Jrnp02u9y2wq",
    "outputId": "11927ed0-d1a2-4be4-da56-8b0e44cf051d"
   },
   "outputs": [],
   "source": [
    "print(label_encoding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6by8ObCLJWD"
   },
   "source": [
    "# Intersect Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81811,
     "status": "ok",
     "timestamp": 1736948455877,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "W_DVPtq4LOfL",
    "outputId": "972087da-04a3-4e2b-9e2f-91642a18fedd"
   },
   "outputs": [],
   "source": [
    "matrix_intersection = {}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#train_df, test_df = train_test_split(data_df, test_size=0.2, random_state=42)\n",
    "\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(data_df)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # split test and train data\n",
    "    train_data = data_df.iloc[train_index]\n",
    "    test_data = data_df.iloc[test_index]\n",
    "\n",
    "    word_encoding_dict = {}\n",
    "    presence_word_dict = {}\n",
    "    label_encoding_dict = {}\n",
    "    num_words = 0\n",
    "\n",
    "    # recreates de dictioanries\n",
    "    for i, (text, label) in enumerate(zip(train_data[text_col], train_data[label_col])):\n",
    "        if label not in label_encoding_dict:\n",
    "            label_encoding_dict[label] = len(label_encoding_dict) + 1\n",
    "        splitted = text.split(\" \")\n",
    "        for word in splitted:\n",
    "            if word not in word_encoding_dict:\n",
    "                num_words += 1\n",
    "                word_encoding_dict[word] = num_words\n",
    "                presence_word_dict[num_words] = set()\n",
    "            presence_word_dict[word_encoding_dict[word]].add(i)\n",
    "\n",
    "    y_pred = np.zeros(len(test_data))\n",
    "    y_true = np.zeros(len(test_data))\n",
    "    # prob_matrix = np.zeros((10000, 5))\n",
    "    prob_matrix = np.zeros((len(test_data), len(label_encoding_dict))) # matrix of probabilities\n",
    "\n",
    "\n",
    "    for i, (text, target) in enumerate(zip(test_data[text_col], test_data[label_col])):\n",
    "        splitted = list(set(text.split(\" \")))\n",
    "        clusters = []\n",
    "        for j in range(len(splitted) - 1):\n",
    "            if splitted[j] not in word_encoding_dict:\n",
    "                continue\n",
    "            a = word_encoding_dict[splitted[j]]\n",
    "            for k in range(j + 1, len(splitted)):\n",
    "                if splitted[k] not in word_encoding_dict:\n",
    "                    continue\n",
    "                b = word_encoding_dict[splitted[k]]\n",
    "                min_index = min(a, b)\n",
    "                max_index = max(a, b)\n",
    "\n",
    "                if min_index in matrix_intersection:\n",
    "                    if max_index in matrix_intersection[min_index]:\n",
    "                        intersection = matrix_intersection[min_index][max_index]\n",
    "                    else:\n",
    "                        intersection = len(presence_word_dict[a] & presence_word_dict[b])\n",
    "                        matrix_intersection[min_index][max_index] = intersection\n",
    "                else:\n",
    "                    matrix_intersection[min_index] = dict()\n",
    "                    intersection = len(presence_word_dict[a] & presence_word_dict[b])\n",
    "                    matrix_intersection[min_index][max_index] = intersection\n",
    "                if intersection > 0:\n",
    "                    clusters.append({\n",
    "                        \"a\": min_index,\n",
    "                        \"b\": max_index,\n",
    "                        \"intersection\": intersection\n",
    "                    })\n",
    "                #if len(intersection) > 0:\n",
    "                #    clusters.append({\n",
    "                #        \"a\": min_index,\n",
    "                #        \"b\": max_index,\n",
    "                #        \"intersection\": len(intersection)\n",
    "                #    })\n",
    "\n",
    "        # Sort the clusters\n",
    "        sorted_clusters = sorted(clusters, key=lambda x: x['intersection'], reverse=True)\n",
    "        # Create the partition\n",
    "        covered_elements = set()\n",
    "        remaining = len(splitted)\n",
    "        partition = []\n",
    "        k = 0.5\n",
    "        probs = {}\n",
    "        for cluster in sorted_clusters:\n",
    "            a = cluster['a']\n",
    "            b = cluster['b']\n",
    "            if a not in covered_elements and b not in covered_elements:\n",
    "                #partition.append(cluster)\n",
    "                covered_elements.add(a)\n",
    "                covered_elements.add(b)\n",
    "                remaining -= 2\n",
    "                counts = train_data.iloc[list(presence_word_dict[a] &\n",
    "                                        presence_word_dict[b])]['Category'].value_counts()\n",
    "                # Compute the probabilities for the cluster\n",
    "                for count in counts.items():\n",
    "                    label = label_encoding_dict[count[0]]\n",
    "                    prob = (k + count[1]) / (2 * k + counts.sum())\n",
    "                    if label in probs:\n",
    "                        probs[label].append(prob)\n",
    "                    else:\n",
    "                        probs[label] = [prob]\n",
    "\n",
    "            if remaining < 1: # There is no one left to add with a intersection\n",
    "                break\n",
    "        # Compute the prior probabilities (log-space to prevent precision issues)\n",
    "        prioris = {}\n",
    "        for label, prob_list in probs.items():\n",
    "            category_name = list(label_encoding_dict.keys())[list(label_encoding_dict.values()).index(label)]\n",
    "            p = 0  # Start with 0 for summation in log-space\n",
    "\n",
    "            # Sum the log-probabilities for the present elements\n",
    "            for prob in prob_list:\n",
    "                p += np.log(prob)\n",
    "            prioris[category_name] = p\n",
    "\n",
    "            # Account for the clusters where the label is absent\n",
    "            absent_clusters = len(partition) - len(prob_list)\n",
    "            if absent_clusters == len(partition):\n",
    "                p = np.log(k / (2 * k))\n",
    "            # if absent_clusters > 0:\n",
    "            #   p += absent_clusters * np.log(k / (2 * k))\n",
    "\n",
    "            prioris[label] = p\n",
    "\n",
    "        # Compute the global prior counts for each label\n",
    "        count_labels = train_data[label_col].value_counts().to_dict()\n",
    "        count_labels = {label_encoding_dict[label]: count_labels[label] for label in count_labels}\n",
    "\n",
    "        # Naive Bayes denominator (in log-space for stability)\n",
    "        log_denominator = None\n",
    "        for label in count_labels:\n",
    "            log_value = prioris[label] + np.log(count_labels[label])\n",
    "            log_denominator = np.logaddexp(log_denominator, log_value) if log_denominator is not None else log_value\n",
    "\n",
    "        # Determine the label with the maximum posterior probability\n",
    "        max_prob = -np.inf\n",
    "        max_label = None\n",
    "        posterioris = {}\n",
    "        for label in count_labels:\n",
    "            category_name = list(label_encoding_dict.keys())[list(label_encoding_dict.values()).index(label)]\n",
    "            log_numerator = prioris[label] + np.log(count_labels[label])\n",
    "            posterior_prob = np.exp(log_numerator - log_denominator)  # Convert back to probability space\n",
    "            posterioris[label] = posterior_prob\n",
    "            if posterior_prob > max_prob:\n",
    "                max_prob = posterior_prob\n",
    "                max_label = category_name\n",
    "        \n",
    "        # Fill the probability matrix\n",
    "        for label, prob in posterioris.items():\n",
    "            if label in label_encoding_dict:\n",
    "                prob_matrix[i, label_encoding_dict[label]] = prob\n",
    "\n",
    "\n",
    "        y_pred[i] = label_encoding_dict[max_label]\n",
    "        y_true[i] = label_encoding_dict[target]\n",
    "\n",
    "        # Result\n",
    "        print(f\"Predicted Label: {max_label}, Probability: {max_prob} | Real: {label_encoding_dict[target]}\")\n",
    "        if i > 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1736948455878,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "Y1fPnJlrR9u6"
   },
   "outputs": [],
   "source": [
    "# y_pred[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1736948455878,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "FaIvpVMxSFO7"
   },
   "outputs": [],
   "source": [
    "# print(\"Categorias e seus respectivos labels:\")\n",
    "# for category, label in label_encoding_dict.items():\n",
    "#     print(f\"Label: {label}, Categoria: {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1736948455879,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "V4gikC3fOCHe",
    "outputId": "4727fc04-ad41-40af-be53-24c1f71d4b7a"
   },
   "outputs": [],
   "source": [
    "# just to print the results\n",
    "\n",
    "num_classes = len(label_encoding_dict)\n",
    "y_true_binary = np.zeros((len(y_true), num_classes))\n",
    "\n",
    "for i, label in enumerate(y_true):\n",
    "  y_true_binary[i, int(label) - 1] = 1\n",
    "\n",
    "\n",
    "prob_matrix = np.exp(prob_matrix) / np.sum(np.exp(prob_matrix), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "roc_auc_scores = [roc_auc_score(y_true_binary[:, i], prob_matrix[:, i]) for i in range(num_classes)]\n",
    "\n",
    "f1 = f1_score(y_true[:100], y_pred[:100], average='weighted', zero_division=0)\n",
    "acc = accuracy_score(y_true[:100], y_pred[:100])\n",
    "recall = recall_score(y_true[:100], y_pred[:100], average='weighted', zero_division=0)\n",
    "precision = precision_score(y_true[:100], y_pred[:100], average='weighted', zero_division=0)\n",
    "\n",
    "print(\"INTERSECT BAYES\")\n",
    "print(f\"Fold {fold + 1} - Precision: {precision:.4f}, Recall: {recall:.4f}, Accuracy: {acc:.4f}, F1: {f1:.4f}, ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "\n",
    "fold_metrics.append({\n",
    "  \"precision\": precision,\n",
    "  \"recall\": recall,\n",
    "  \"accuracy\": acc,\n",
    "  \"f1\": f1,\n",
    "  \"roc_auc\": np.mean(roc_auc_scores)\n",
    "})\n",
    "\n",
    "\n",
    "final_metrics = {metric: np.mean([fold[metric] for fold in fold_metrics]) for metric in fold_metrics[0]}\n",
    "print(\"\\nResultados finais (Média dos 5 Folds):\")\n",
    "for metric, value in final_metrics.items():\n",
    "  print(f\"{metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75iC7FNKu5Au"
   },
   "source": [
    "# Multinomial Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1736948455879,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "6j1HhdlovH7J"
   },
   "outputs": [],
   "source": [
    "def CountOrTfidf_vec(train_texts, test_texts, method):\n",
    "    if method == \"count\":\n",
    "        vectorizer = CountVectorizer(min_df=5, max_features=30000)\n",
    "    elif method == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(min_df=5, max_features=30000)\n",
    "    else:\n",
    "        raise ValueError(\"O método deve ser 'count' ou 'tfidf'\")\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "    x_test = vectorizer.transform(test_texts)\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5549,
     "status": "ok",
     "timestamp": 1736948461418,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "rv5fr_XbvTOo",
    "outputId": "4a76a41c-37fd-4a69-cf32-8d5d5b764672"
   },
   "outputs": [],
   "source": [
    "f1_mnb, auc_mnb, acc_mnb, recall_mnb, precision_mnb = (\n",
    "    np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5)\n",
    ")\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(data_df)):\n",
    "    print(f\"Fold {fold + 1}/5\")\n",
    "\n",
    "    train = data_df.iloc[train_index]\n",
    "    test = data_df.iloc[test_index].head(500) # limit the number of samples\n",
    "\n",
    "    x_trainCorT, x_testCorT = CountOrTfidf_vec(train[text_col], test[text_col], \"count\")\n",
    "\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(x_trainCorT, train[label_col])\n",
    "\n",
    "    pred_mnb = nb.predict(x_testCorT)\n",
    "    pred_proba_mnb = nb.predict_proba(x_testCorT)\n",
    "\n",
    "    f1_mnb[fold] = f1_score(test[label_col], pred_mnb, average=\"weighted\")\n",
    "    auc_mnb[fold] = roc_auc_score(test[label_col], pred_proba_mnb, multi_class=\"ovr\")\n",
    "    acc_mnb[fold] = accuracy_score(test[label_col], pred_mnb)\n",
    "    recall_mnb[fold] = recall_score(test[label_col], pred_mnb, average=\"weighted\")\n",
    "    precision_mnb[fold] = precision_score(test[label_col], pred_mnb, average=\"weighted\")\n",
    "\n",
    "\n",
    "print(\"MULTINOMIAL NAIVE BAYES\")\n",
    "print(f\"F1-Score: {np.mean(f1_mnb):.4f}\")\n",
    "print(f\"ROC-AUC: {np.mean(auc_mnb):.4f}\")\n",
    "print(f\"Accuracy: {np.mean(acc_mnb):.4f}\")\n",
    "print(f\"Recall: {np.mean(recall_mnb):.4f}\")\n",
    "print(f\"Precision: {np.mean(precision_mnb):.4f}\")\n",
    "\n",
    "\n",
    "#break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_9JcLRN5Taz"
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20701,
     "status": "ok",
     "timestamp": 1736948482106,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "6z5nsmsAIgs8",
    "outputId": "1eb4c710-baa3-4e08-eefd-28b11abb570c"
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_metrics = []\n",
    "\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(data_df)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # split test and train data\n",
    "    train_data = data_df.iloc[train_index]\n",
    "    test_data = data_df.iloc[test_index]\n",
    "\n",
    "    # create dictionaries\n",
    "    word_freq_per_class = {}\n",
    "    class_freq = {}\n",
    "    total_words_per_class = {}\n",
    "    vocab = set()\n",
    "\n",
    "    # Construir frequências com os dados de treino\n",
    "    for text, label in zip(train_data[text_col], train_data[label_col]):\n",
    "        if label not in word_freq_per_class:\n",
    "            word_freq_per_class[label] = {}\n",
    "            class_freq[label] = 0\n",
    "            total_words_per_class[label] = 0\n",
    "\n",
    "        class_freq[label] += 1\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            vocab.add(word)\n",
    "            if word not in word_freq_per_class[label]:\n",
    "                word_freq_per_class[label][word] = 0\n",
    "            word_freq_per_class[label][word] += 1\n",
    "            total_words_per_class[label] += 1\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    k = 0.5  # Laplace smoothing\n",
    "\n",
    "    # predict vectors\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for i, (text, true_label) in enumerate(zip(test_data[text_col], test_data[label_col])):\n",
    "        # if i >= 100:\n",
    "        #   break\n",
    "        words = text.split()\n",
    "\n",
    "        # calculate the probabilities for each class\n",
    "        class_probabilities = {}\n",
    "        for label in class_freq:\n",
    "            log_prob = np.log(class_freq[label] / len(train_data))  # probability prior\n",
    "\n",
    "            for word in words:\n",
    "                word_count = word_freq_per_class[label].get(word, 0)\n",
    "                # conditional probability\n",
    "                log_prob += np.log((word_count + k) / (total_words_per_class[label] + k * vocab_size))\n",
    "\n",
    "            class_probabilities[label] = log_prob\n",
    "\n",
    "        # choose class with highest probability\n",
    "        predicted_label = max(class_probabilities, key=class_probabilities.get)\n",
    "        predicted_probability = np.exp(class_probabilities[label])\n",
    "        y_pred.append(predicted_label)\n",
    "        y_true.append(true_label)\n",
    "\n",
    "        print(f\"Previsto: {predicted_label}, Real: {true_label}\")\n",
    "\n",
    "        if i > 500:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1736948482107,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "G5OR4Q-GG0wP",
    "outputId": "66cd6339-e5ba-4d67-bc03-708aa0635d1e"
   },
   "outputs": [],
   "source": [
    "classes = list(class_freq.keys())\n",
    "y_true_bin = label_binarize(y_true, classes=classes)\n",
    "y_pred_bin = label_binarize(y_pred, classes=classes)\n",
    "\n",
    "roc_auc_nb = roc_auc_score(y_true_bin, y_pred_bin, average=\"macro\", multi_class=\"ovr\")\n",
    "\n",
    "f1_nb = f1_score(y_true[:100], y_pred[:100], average=\"weighted\", zero_division=0)\n",
    "acc_nb = accuracy_score(y_true[:100], y_pred[:100])\n",
    "recall_nb = recall_score(y_true[:100], y_pred[:100], average=\"weighted\", zero_division=0)\n",
    "precision_nb = precision_score(y_true[:100], y_pred[:100], average=\"weighted\", zero_division=0)\n",
    "\n",
    "fold_metrics.append({\n",
    "  \"f1\": f1_nb,\n",
    "  \"accuracy\": acc_nb,\n",
    "  \"recall\": recall_nb,\n",
    "  \"precision\": precision_nb,\n",
    "  \"roc_auc\": roc_auc_nb\n",
    "    })\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame(fold_metrics)\n",
    "\n",
    "\n",
    "print(\"\\nMétricas médias:\")\n",
    "print(metrics_df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFTUqBxFIL3u"
   },
   "source": [
    "# Resultado final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1736948515402,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "BurkPkKfIQzj",
    "outputId": "51e5a7cd-a6da-4f38-b2a3-49aa31ddfa54"
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Metricas com todas as amostras\": [\"Precision\", \"Recall\", \"Accuracy\", \"F1\", \"ROC AUC\"],\n",
    "\n",
    "    \"Intersect Bayes\": [\n",
    "        f\"{precision:.4f}\",\n",
    "        f\"{recall:.4f}\",\n",
    "        f\"{acc:.4f}\",\n",
    "        f\"{f1:.4f}\",\n",
    "        f\"{np.mean(roc_auc_scores):.4f}\",\n",
    "    ],\n",
    "\n",
    "    \"Naive Bayes\": [\n",
    "        f\"{precision_nb:.4f}\",\n",
    "        f\"{recall_nb:.4f}\",\n",
    "        f\"{acc_nb:.4f}\",\n",
    "        f\"{f1_nb:.4f}\",\n",
    "        f\"{roc_auc_nb:.4f}\",\n",
    "    ],\n",
    "\n",
    "    \"Multinomial Naive Bayes\": [\n",
    "        f\"{np.mean(precision_mnb):.4f}\",\n",
    "        f\"{np.mean(recall_mnb):.4f}\",\n",
    "        f\"{np.mean(acc_mnb):.4f}\",\n",
    "        f\"{np.mean(f1_mnb):.4f}\",\n",
    "        f\"{np.mean(auc_mnb):.4f}\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_results\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
