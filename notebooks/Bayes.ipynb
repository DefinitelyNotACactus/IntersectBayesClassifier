{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "VzRsyfCuBe8_",
   "metadata": {
    "id": "VzRsyfCuBe8_"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c90a452",
   "metadata": {
    "executionInfo": {
     "elapsed": 21238,
     "status": "ok",
     "timestamp": 1737517146468,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "0c90a452"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import json\n",
    "# import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from collections import defaultdict\n",
    "import scipy.special as sc\n",
    "\n",
    "import warnings\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, CategoricalNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d878432",
   "metadata": {
    "id": "6d878432"
   },
   "outputs": [],
   "source": [
    "def kde(column, target, data, ax=None, title=None, xlabel=\"\", ylabel=\"\"):\n",
    "    if ax is None: fig, ax = plt.subplots(1, 1, figsize=(15, 3))\n",
    "    g = sns.kdeplot(data[column][(data[target] == 'ham')], ax=ax, color=\"Red\")\n",
    "    g = sns.kdeplot(data[column][(data[target] == 'spam')], ax=g, color=\"Blue\")\n",
    "    g.set_xlabel(xlabel)\n",
    "    g.set_ylabel(ylabel)\n",
    "    if title is not None: g.set_title(title)\n",
    "    #g = g.legend(['Não conluio', 'Conluio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9143587",
   "metadata": {
    "id": "b9143587"
   },
   "source": [
    "## Licitações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5f9fb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "executionInfo": {
     "elapsed": 486,
     "status": "error",
     "timestamp": 1717852913575,
     "user": {
      "displayName": "David",
      "userId": "07163654608838514179"
     },
     "user_tz": 180
    },
    "id": "5d5f9fb2",
    "outputId": "c1f11c16-d70c-4cb8-f9b0-9a112930d812"
   },
   "outputs": [],
   "source": [
    "country = \"Italy\"\n",
    "\n",
    "collusion = pd.read_csv(f\"1-s2/DB_Collusion_{country}_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a9105",
   "metadata": {
    "id": "b44a9105"
   },
   "outputs": [],
   "source": [
    "tenders = {}\n",
    "labels = {}\n",
    "for row in collusion.itertuples():\n",
    "    if row.Tender in tenders:\n",
    "        tenders[row.Tender] += f\" {row.Competitors}\"\n",
    "    else:\n",
    "        tenders[row.Tender] = f\"{row.Competitors}\"\n",
    "        labels[row.Tender] = row.Collusive_competitor_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68acfdaf",
   "metadata": {
    "id": "68acfdaf"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"Competitors\": tenders, \"Collusive\": labels})\n",
    "data['id'] = data.index\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d5b8b0",
   "metadata": {
    "id": "59d5b8b0"
   },
   "source": [
    "## Carregar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5STY97H7kOZZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 1477,
     "status": "ok",
     "timestamp": 1737518580757,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "5STY97H7kOZZ",
    "outputId": "d83d3bd2-ba17-440c-c3ef-6d9f453767ae"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/MyDrive/Mestrado_códigos/SpamCode/emailNPL/notebooks\"\n",
    "\n",
    "files = {\n",
    "    \"email\": \"CleanEmail\",\n",
    "}\n",
    "\n",
    "data = pd.read_csv(f\"{path}/{files['email']}.csv\", index_col=0).dropna().reset_index(drop=True)\n",
    "# data['id'] = data.index\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9c9YnLN0oO5",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1737518594152,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "a9c9YnLN0oO5"
   },
   "outputs": [],
   "source": [
    "# Atualize aqui o nome da coluna de texto e rótulo\n",
    "text = 'Email'\n",
    "#text = \"Competitors\"\n",
    "#target = \"Collusive\"\n",
    "#data[target] = data[target].apply(lambda x: 'ham' if x == 0 else 'spam')\n",
    "target = 'Spam/Ham'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1yPO5ig2yVrD",
   "metadata": {
    "id": "1yPO5ig2yVrD"
   },
   "source": [
    "### K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610d734",
   "metadata": {
    "id": "e610d734"
   },
   "outputs": [],
   "source": [
    "K = 5\n",
    "data['fold'] = [i % K for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1lpBD2HMBuI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1737485215547,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "d1lpBD2HMBuI",
    "outputId": "90d24f38-5e42-4e92-af68-6335cadfd62f"
   },
   "outputs": [],
   "source": [
    "\n",
    "word_number = [len(doc.split()) for doc in data[text]]\n",
    "\n",
    "result_word = sum(word_number)\n",
    "\n",
    "print(f\"total de palavras: {result_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pjXfrUa-MRhX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2691,
     "status": "ok",
     "timestamp": 1737485270738,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "pjXfrUa-MRhX",
    "outputId": "a047ce33-cefb-4e1a-8e1d-039ed70d4441"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MmyqV3WCyDjJ",
   "metadata": {
    "id": "MmyqV3WCyDjJ"
   },
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yIAyD1EYn3zy",
   "metadata": {
    "id": "yIAyD1EYn3zy"
   },
   "outputs": [],
   "source": [
    "# Train / test split\n",
    "np.random.seed(42)\n",
    "\n",
    "#dict2_bidding = df['l'].unique()\n",
    "\n",
    "train_indices = np.random.choice(len(data), int(np.round(len(data) * 0.8)), replace=False)\n",
    "\n",
    "train = data.loc[data['id'].isin(train_indices), :]\n",
    "test = data.loc[~data['id'].isin(train_indices), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pGVUROJr5S5I",
   "metadata": {
    "id": "pGVUROJr5S5I"
   },
   "source": [
    "## Create Words Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5tlD8At85Uc-",
   "metadata": {
    "id": "5tlD8At85Uc-"
   },
   "outputs": [],
   "source": [
    "# count words without max_feature. 0 = non-spam and 1 = spam\n",
    "def create_words_dict(train, text, target):\n",
    "    words_dict_0, words_dict_1, words_dict = dict(), dict(), dict()\n",
    "\n",
    "    for i, (entry, yi) in enumerate(zip(train[text], train[target])):\n",
    "        for word in set(entry.split(\" \")):\n",
    "            if word in words_dict:\n",
    "                words_dict[word].add(i)\n",
    "            else:\n",
    "                words_dict[word] = {i}\n",
    "            if yi == 'ham':\n",
    "                if word in words_dict_0:\n",
    "                    words_dict_0[word].add(i)\n",
    "                else:\n",
    "                    words_dict_0[word] = {i}\n",
    "            else:\n",
    "                if word in words_dict_1:\n",
    "                    words_dict_1[word].add(i)\n",
    "                else:\n",
    "                    words_dict_1[word] = {i}\n",
    "\n",
    "    print(f\"words_dict_0: {len(words_dict_0)} words_dict_1:{len(words_dict_1)}\")\n",
    "\n",
    "    return words_dict_1, words_dict_0, words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5DW8fEV_5Vn0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6954,
     "status": "ok",
     "timestamp": 1730228364451,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "5DW8fEV_5Vn0",
    "outputId": "d3a05997-ba66-48b9-b9e9-0ce304927600"
   },
   "outputs": [],
   "source": [
    "words_dict_1, words_dict_0, words_dict = create_words_dict(data, text, target)\n",
    "vocabulary_size = len(words_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knEaviaazk3c",
   "metadata": {
    "id": "knEaviaazk3c"
   },
   "source": [
    "## Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dhFg3XQ152zU",
   "metadata": {
    "id": "dhFg3XQ152zU"
   },
   "outputs": [],
   "source": [
    "max_len = data[text].apply(lambda x: len(x.split(\" \"))).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sXwoc9S21zck",
   "metadata": {
    "id": "sXwoc9S21zck"
   },
   "source": [
    "#### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464Yu9m10sv",
   "metadata": {
    "id": "5464Yu9m10sv"
   },
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06_HtpxjCgTc",
   "metadata": {
    "id": "06_HtpxjCgTc"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qx3w8riQ7uKH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3198,
     "status": "ok",
     "timestamp": 1717854208268,
     "user": {
      "displayName": "David",
      "userId": "07163654608838514179"
     },
     "user_tz": 180
    },
    "id": "qx3w8riQ7uKH",
    "outputId": "1b062977-0bbc-46ee-ac92-5d1209287410"
   },
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and one dense layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size + 1,\n",
    "                    25,\n",
    "                    #weights=[embedding_matrix],\n",
    "                    input_length=max_len,\n",
    "                    trainable=False))\n",
    "\n",
    "model.add(LSTM(50, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67TKm0DGELhh",
   "metadata": {
    "id": "67TKm0DGELhh"
   },
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = txt.Tokenizer(num_words=None)\n",
    "\n",
    "token.fit_on_texts(list(train[text]) + list(test[text]))\n",
    "\n",
    "xtrain_seq = token.texts_to_sequences(train[text])\n",
    "xvalid_seq = token.texts_to_sequences(test[text])\n",
    "\n",
    "#zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CeTBOEbP_1Fm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeTBOEbP_1Fm",
    "outputId": "e1604f6b-66e7-4240-be5e-aa6fa625053e"
   },
   "outputs": [],
   "source": [
    "model.fit(xtrain_pad, np.array(train[target].apply(lambda x: 1 if x == 'spam' else 0).values), epochs=5, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zf7CUnSnyJ6w",
   "metadata": {
    "id": "zf7CUnSnyJ6w"
   },
   "source": [
    "## IntersectBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb7d05b",
   "metadata": {
    "id": "3fb7d05b"
   },
   "outputs": [],
   "source": [
    "# Função para processar o particionamento da versão exata\n",
    "def solution_clusters(js):\n",
    "    remaining = set(entry.split(' '))\n",
    "    solution = []\n",
    "    for cluster in js['Clusters']:\n",
    "        if len(cluster['Intersection']) > 0:\n",
    "            a = entry_dict[cluster['Elements'][0]]\n",
    "            b = entry_dict[cluster['Elements'][1]]\n",
    "            intersection_0 = intersection_1 = set()\n",
    "            if a in words_dict_0 and b in words_dict_0:\n",
    "                intersection_0 = words_dict_0[a] & words_dict_0[b]\n",
    "            if a in words_dict_1 and b in words_dict_1:\n",
    "                intersection_1 = words_dict_1[a] & words_dict_1[b]\n",
    "            solution.append({\n",
    "                'a': a,\n",
    "                'b': b,\n",
    "                'int': len(cluster['Intersection']),\n",
    "                'int_0': len(intersection_0),\n",
    "                'int_1': len(intersection_1)\n",
    "            })\n",
    "            for element in cluster['Elements']:\n",
    "                remaining.remove(entry_dict[element])\n",
    "            if len(remaining) <= 1:\n",
    "                break\n",
    "\n",
    "    for word in remaining:\n",
    "        if word in words_dict:\n",
    "            solution.append({\n",
    "                    'a': word,\n",
    "                    'b': word,\n",
    "                    'int': len(words_dict[word]),\n",
    "                    'int_0': len(words_dict_0[word]) if word in words_dict_0 else 0,\n",
    "                    'int_1': len(words_dict_1[word]) if word in words_dict_1 else 0\n",
    "                })\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4c516",
   "metadata": {
    "id": "87b4c516"
   },
   "outputs": [],
   "source": [
    "# Classe que cria os candidatos a serem grupos\n",
    "class Pairs:\n",
    "    def __init__(self, words_dict, words_dict_0, words_dict_1):\n",
    "        self.pairs_matrix = dict()\n",
    "        self.words_dict = words_dict\n",
    "        self.words_dict_1 = words_dict_1\n",
    "        self.words_dict_0 = words_dict_0\n",
    "\n",
    "    def get_pairs(self, entry):\n",
    "        pairs = []\n",
    "        words = list(set(entry.split(' ')))\n",
    "        for j in range(len(words)):\n",
    "            for k in range(j+1, len(words)):\n",
    "                if words[j] not in self.words_dict or words[k] not in self.words_dict:\n",
    "                    continue\n",
    "                if words[j] in self.pairs_matrix and words[k] in self.pairs_matrix[words[j]]:\n",
    "                    if self.pairs_matrix[words[j]][words[k]][0] > 0:\n",
    "                        pairs.append({\n",
    "                            'a': words[j],\n",
    "                            'b': words[k],\n",
    "                            'int': self.pairs_matrix[words[j]][words[k]][0],\n",
    "                            'int_0': self.pairs_matrix[words[j]][words[k]][1],\n",
    "                            'int_1': self.pairs_matrix[words[j]][words[k]][2]\n",
    "                        })\n",
    "                    continue\n",
    "\n",
    "                intersection = self.words_dict[words[j]] & self.words_dict[words[k]]\n",
    "                intersection_0, intersection_1 = dict(), dict()\n",
    "                if words[j] in self.words_dict_0 and words[k] in self.words_dict_0:\n",
    "                    intersection_0 = words_dict_0[words[j]] & self.words_dict_0[words[k]]\n",
    "                if words[j] in self.words_dict_1 and words[k] in self.words_dict_1:\n",
    "                    intersection_1 = self.words_dict_1[words[j]] & self.words_dict_1[words[k]]\n",
    "\n",
    "                vec = [len(intersection), len(intersection_0), len(intersection_1)]\n",
    "                if words[j] not in self.pairs_matrix:\n",
    "                    self.pairs_matrix[words[j]] = {words[k]: vec}\n",
    "                else:\n",
    "                    self.pairs_matrix[words[j]][words[k]] = vec\n",
    "                if words[k] not in self.pairs_matrix:\n",
    "                    self.pairs_matrix[words[k]] = {words[j]: vec}\n",
    "                else:\n",
    "                    self.pairs_matrix[words[k]][words[j]] = vec\n",
    "\n",
    "                if len(intersection) > 0:\n",
    "                    pairs.append({\n",
    "                        'a': words[j],\n",
    "                        'b': words[k],\n",
    "                        'int': self.pairs_matrix[words[j]][words[k]][0],\n",
    "                        'int_0': self.pairs_matrix[words[j]][words[k]][1],\n",
    "                        'int_1': self.pairs_matrix[words[j]][words[k]][2]\n",
    "                    })\n",
    "\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c9e68",
   "metadata": {
    "id": "a18c9e68"
   },
   "outputs": [],
   "source": [
    "# Heuristica gulosa\n",
    "# pairs = candidatos, supoe-se que está ordenado pelo tamanho da interseção\n",
    "# entry = texto do email\n",
    "def get_solution(pairs, entry):\n",
    "    remaining = set(entry.split(' '))\n",
    "    solution = []\n",
    "    # Se houver grupos\n",
    "    if len(pairs) > 0:\n",
    "        # Para cada grupo\n",
    "        for row in pairs.itertuples():\n",
    "            # Se o grupo pode ser adicionado no particionamento, o adicone\n",
    "            if row.a in remaining and row.b in remaining:\n",
    "                solution.append({\n",
    "                    'a': row.a,\n",
    "                    'b': row.b,\n",
    "                    'int': row.int,\n",
    "                    'int_0': row.int_0,\n",
    "                    'int_1': row.int_1\n",
    "                })\n",
    "                remaining.remove(row.a)\n",
    "                remaining.remove(row.b)\n",
    "                if len(remaining) <= 1:\n",
    "                    break\n",
    "\n",
    "    # Caso existam palavras fora do particionamento, criar grupos unitários para elas.\n",
    "    # Tanto faz criar um único ou vários, não muda a função objetivo\n",
    "    for word in remaining:\n",
    "        if word in words_dict:\n",
    "            solution.append({\n",
    "                    'a': word,\n",
    "                    'b': word,\n",
    "                    'int': len(words_dict[word]),\n",
    "                    'int_0': len(words_dict_0[word]) if word in words_dict_0 else 0,\n",
    "                    'int_1': len(words_dict_1[word]) if word in words_dict_1 else 0\n",
    "                })\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42746b8",
   "metadata": {
    "id": "c42746b8"
   },
   "source": [
    "### Predição utilizando particionamentos $P^{spam}$ e $P^{ham}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48252e6d",
   "metadata": {
    "id": "48252e6d",
    "outputId": "bbf7b39b-3a16-41c9-b672-0d44800cbc9a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 0.001\n",
    "\n",
    "f1_2, auc_2, acc_2, recall_2, precision_2 = (\n",
    "    np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5)\n",
    ")\n",
    "\n",
    "for fold in range(5):\n",
    "    break\n",
    "    print(f\"Fold {fold}/5\")\n",
    "\n",
    "    train = data.loc[data['fold'] != fold, :]\n",
    "    test = data.loc[data['fold'] == fold, :]\n",
    "\n",
    "    words_dict_1, words_dict_0, words_dict = create_words_dict(train, text, target)\n",
    "    obj_pairs_0 = Pairs(words_dict, words_dict_0, {})\n",
    "    obj_pairs_1 = Pairs(words_dict, {}, words_dict_1)\n",
    "\n",
    "    prioris = train[target].value_counts(normalize=True)\n",
    "    counts = train[target].value_counts(normalize=False)\n",
    "\n",
    "    xtest_prob_0, xtest_prob_1 = np.zeros(len(test)), np.zeros(len(test))\n",
    "    solutions = np.zeros(len(test))\n",
    "    for i, entry in enumerate(test[text]):\n",
    "        print(f\"{i}/{len(test)}\")\n",
    "        try:\n",
    "            pairs_0 = obj_pairs_0.get_pairs(entry)\n",
    "            pairs_1 = obj_pairs_1.get_pairs(entry)\n",
    "\n",
    "            pairs_0 = pd.DataFrame(pairs_0)\n",
    "            pairs_1 = pd.DataFrame(pairs_1)\n",
    "            if len(pairs_0) > 0:\n",
    "                pairs_0 = pairs_0.sort_values(by='int', ascending=False)\n",
    "            if len(pairs_1) > 0:\n",
    "                pairs_1 = pairs_1.sort_values(by='int', ascending=False)\n",
    "\n",
    "            solution_0 = pd.DataFrame(get_solution(pairs_0, entry)).sort_values(by='int', ascending=False)\n",
    "            solution_1 = pd.DataFrame(get_solution(pairs_1, entry)).sort_values(by='int', ascending=False)\n",
    "\n",
    "            priori_words_0 = priori_words_1 = 1\n",
    "            solutions[i] = len(solution_0) + len(solution_1)\n",
    "            for row in solution_0.itertuples():\n",
    "                    priori_words_0 *= (k + row.int_0) / (2*k + row.int)\n",
    "            for row in solution_1.itertuples():\n",
    "                    priori_words_1 *= (k + row.int_1) / (2*k + row.int)\n",
    "\n",
    "            xtest_prob_0[i] = (priori_words_0 * prioris['ham']) / (priori_words_0 * prioris['ham'] +\n",
    "                                                                priori_words_1 * prioris['spam'])\n",
    "            xtest_prob_1[i] = (priori_words_1 * prioris['spam']) / (priori_words_0 * prioris['ham'] +\n",
    "                                                                 priori_words_1 * prioris['spam'])\n",
    "            #print(f\"I = {solution['int'].sum()} I_0 = {solution['int_0'].sum()} I_1 = {solution['int_1'].sum()}\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            #print(f\"Can't label {i}, '{entry}' words not present in train\")\n",
    "\n",
    "     # Evaluate\n",
    "    pred = xtest_prob_0 < xtest_prob_1\n",
    "    pred_proba = np.nan_to_num(xtest_prob_1, nan=0.5)\n",
    "    f1_2[fold] = f1_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred)\n",
    "    auc_2[fold] = roc_auc_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred_proba)\n",
    "    acc_2[fold] = accuracy_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred)\n",
    "    recall_2[fold] = recall_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred)\n",
    "    precision_2[fold] = precision_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f84a73",
   "metadata": {
    "id": "87f84a73",
    "outputId": "f57ca8ed-a9dc-4cea-d765-cce378cf9d93"
   },
   "outputs": [],
   "source": [
    "print(f\"Precision: {precision_2.mean():.4f} +/- {precision_2.std():.4f}\")\n",
    "print(f\"Recall: {recall_2.mean():.4f} +/- {recall_2.std():.4f}\")\n",
    "print(f\"Accuracy: {acc_2.mean():.4f} +/- {acc_2.std():.4f}\")\n",
    "print(f\"F1: {f1_2.mean():.4f} +/- {f1_2.std():.4f}\")\n",
    "print(f\"ROC AUC: {auc_2.mean():.4f} +/- {auc_2.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c0b277",
   "metadata": {
    "id": "b3c0b277"
   },
   "source": [
    "### Predição utilizando particionamento único"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cea666",
   "metadata": {
    "id": "68cea666",
    "outputId": "53d0e63c-fb2e-4165-82c8-d3d2c848acbb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#intersect bayes\n",
    "%%time\n",
    "k = 0.001\n",
    "\n",
    "f1, auc, acc, recall, precision = (\n",
    "    np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5)\n",
    ")\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"Fold {fold}/5\")\n",
    "\n",
    "    train = data.loc[data['fold'] != fold, :]\n",
    "    test = data.loc[data['fold'] == fold, :]\n",
    "\n",
    "    words_dict_1, words_dict_0, words_dict = create_words_dict(train, text, target)\n",
    "    obj_pairs = Pairs(words_dict, words_dict_0, words_dict_1)\n",
    "\n",
    "    prioris = train[target].value_counts(normalize=True)\n",
    "    counts = train[target].value_counts(normalize=False)\n",
    "\n",
    "    xtest_prob_0, xtest_prob_1 = np.zeros(len(test)), np.zeros(len(test))\n",
    "    solutions = np.zeros(len(test))\n",
    "    for i, entry in enumerate(test[text]):\n",
    "        print(f\"{i}/{len(test)}\")\n",
    "        try:\n",
    "            pairs = pd.DataFrame(obj_pairs.get_pairs(entry))\n",
    "            if len(pairs) > 0:\n",
    "                pairs = pairs.sort_values(by='int', ascending=False)\n",
    "            solution = pd.DataFrame(get_solution(pairs, entry)).sort_values(by='int', ascending=False)\n",
    "            priori_words_0 = priori_words_1 = 1\n",
    "            solutions[i] = len(solution)\n",
    "            for row in solution.itertuples():\n",
    "                    priori_words_0 *= (k + row.int_0) / (2*k + row.int)\n",
    "                    priori_words_1 *= (k + row.int_1) / (2*k + row.int)\n",
    "\n",
    "            xtest_prob_0[i] = (priori_words_0 * prioris['ham']) / (priori_words_0 * prioris['ham'] +\n",
    "                                                                priori_words_1 * prioris['spam'])\n",
    "            xtest_prob_1[i] = (priori_words_1 * prioris['spam']) / (priori_words_0 * prioris['ham'] +\n",
    "                                                                 priori_words_1 * prioris['spam'])\n",
    "            #print(f\"I = {solution['int'].sum()} I_0 = {solution['int_0'].sum()} I_1 = {solution['int_1'].sum()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Can't label {i}, '{entry}' words not present in train\")\n",
    "\n",
    "     # Evaluate\n",
    "    pred = xtest_prob_0 < xtest_prob_1\n",
    "    pred_proba = np.nan_to_num(xtest_prob_1, nan=0.5)\n",
    "    f1[fold] = f1_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred)\n",
    "    auc[fold] = roc_auc_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred_proba)\n",
    "    acc[fold] = accuracy_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred)\n",
    "    recall[fold] = recall_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred)\n",
    "    precision[fold] = precision_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3405af",
   "metadata": {
    "id": "0d3405af",
    "outputId": "0440477a-0881-473b-e5a1-25dd8f876f2a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Recall: {recall.mean():.4f} +/- {recall.std():.4f}\")\n",
    "print(f\"Precision: {precision.mean():.4f} +/- {precision.std():.4f}\")\n",
    "print(f\"Accuracy: {acc.mean():.4f} +/- {acc.std():.4f}\")\n",
    "print(f\"F1: {f1.mean():.4f} +/- {f1.std():.4f}\")\n",
    "print(f\"ROC AUC: {auc.mean():.4f} +/- {auc.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123953c2",
   "metadata": {
    "id": "123953c2",
    "outputId": "021aada1-766c-4467-b90e-a3e2b8ed3717",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#naive bayes\n",
    "%%time\n",
    "k = 0.001\n",
    "\n",
    "f1_pure, auc_pure, acc_pure, recall_pure, precision_pure = (\n",
    "    np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5)\n",
    ")\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"Fold {fold}/5\")\n",
    "\n",
    "    train = data.loc[data['fold'] != fold, :]\n",
    "    test = data.loc[data['fold'] == fold, :]\n",
    "\n",
    "    words_dict_1, words_dict_0, words_dict = create_words_dict(train, text, target)\n",
    "\n",
    "    prioris = train[target].value_counts(normalize=True)\n",
    "    counts = train[target].value_counts(normalize=False)\n",
    "\n",
    "    xtest_prob_0_pure, xtest_prob_1_pure = np.zeros(len(test)), np.zeros(len(test))\n",
    "\n",
    "    for i, entry in enumerate(test[text]):\n",
    "        priori_words_0 = priori_words_1 = 1\n",
    "        for word in set(entry.split(\" \")):\n",
    "            if word in words_dict_0:\n",
    "                priori_words_0 *= (k + len(words_dict_0[word])) / (2*k + counts[0])\n",
    "            else:\n",
    "                priori_words_0 *= k / 2*k\n",
    "            if word in words_dict_1:\n",
    "                priori_words_1 *= (k + len(words_dict_1[word])) / (2*k + counts[1])\n",
    "            else:\n",
    "                priori_words_1 *= k / 2*k\n",
    "\n",
    "        xtest_prob_0_pure[i] = (priori_words_0 * prioris['ham']) / (priori_words_0 * prioris['ham'] +\n",
    "                                                               priori_words_1 * prioris['spam'])\n",
    "        xtest_prob_1_pure[i] = (priori_words_1 * prioris['spam']) / (priori_words_0 * prioris['ham'] +\n",
    "                                                                priori_words_1 * prioris['spam'])\n",
    "    # Evaluate\n",
    "    pred_pure = xtest_prob_0_pure < xtest_prob_1_pure\n",
    "    pred_proba_pure = np.nan_to_num(xtest_prob_1_pure, nan=0.5)\n",
    "    f1_pure[fold] = f1_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred_pure)\n",
    "    auc_pure[fold] = roc_auc_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred_proba_pure)\n",
    "    acc_pure[fold] = accuracy_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred_pure)\n",
    "    recall_pure[fold] = recall_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred_pure)\n",
    "    precision_pure[fold] = precision_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred_pure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7348776",
   "metadata": {
    "id": "a7348776",
    "outputId": "bc14da6c-4a28-4e29-888b-3edb998e4619"
   },
   "outputs": [],
   "source": [
    "print(f\"Recall: {recall_pure.mean():.4f} +/- {recall_pure.std():.4f}\")\n",
    "print(f\"Precision: {precision_pure.mean():.4f} +/- {precision_pure.std():.4f}\")\n",
    "print(f\"Accuracy: {acc_pure.mean():.4f} +/- {acc_pure.std():.4f}\")\n",
    "print(f\"F1: {f1_pure.mean():.4f} +/- {f1_pure.std():.4f}\")\n",
    "print(f\"ROC AUC: {auc_pure.mean():.4f} +/- {auc_pure.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc0b60",
   "metadata": {
    "id": "2cdc0b60"
   },
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313760fe",
   "metadata": {
    "id": "313760fe"
   },
   "outputs": [],
   "source": [
    "def CountOrTfidf_vec(train,test,opt):\n",
    "#does the transformation for given type of vectorizer\n",
    "    if(opt == \"tfidf\"):\n",
    "        transformer = TfidfVectorizer(min_df=5, max_features=30000)\n",
    "    elif(opt == \"count\"):\n",
    "        transformer = CountVectorizer(min_df=5, max_features=30000)\n",
    "    train_CorTf = transformer.fit_transform(train)\n",
    "    test_CorTf = transformer.transform(test)\n",
    "    return train_CorTf,test_CorTf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65ff4a",
   "metadata": {
    "id": "4b65ff4a",
    "outputId": "18d12d0d-97b3-4412-ca74-bf14bf2dd2a1"
   },
   "outputs": [],
   "source": [
    "# multinomail naive bayes\n",
    "f1_mnb, auc_mnb, acc_mnb, recall_mnb, precision_mnb = (\n",
    "    np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5)\n",
    ")\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"Fold {fold}/5\")\n",
    "\n",
    "    train = data.loc[data['fold'] != fold, :]\n",
    "    test = data.loc[data['fold'] == fold, :]\n",
    "\n",
    "    x_trainCorT, x_testCorT = CountOrTfidf_vec(train[text], test[text], \"count\")\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(x_trainCorT.toarray(), train[target].apply(lambda x: 1 if x == 'spam' else 0))\n",
    "\n",
    "    pred_mnb = nb.predict(x_testCorT.toarray())\n",
    "    pred_proba_mnb = nb.predict_proba(x_testCorT.toarray())[:, 1]\n",
    "\n",
    "    f1_mnb[fold] = f1_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred_mnb)\n",
    "    auc_mnb[fold] = roc_auc_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred_proba_mnb)\n",
    "    acc_mnb[fold] = accuracy_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred_mnb)\n",
    "    recall_mnb[fold] = recall_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred_mnb)\n",
    "    precision_mnb[fold] = precision_score(test[target].apply(lambda x: True if x == 'spam' else False),\n",
    "                             pred_mnb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570d48d",
   "metadata": {
    "id": "1570d48d",
    "outputId": "bce090d2-0f26-4c35-dfc4-9299c291d0a5"
   },
   "outputs": [],
   "source": [
    "print(f\"Precision: {precision_mnb.mean():.4f} +/- {precision_mnb.std():.4f}\")\n",
    "print(f\"Recall: {recall_mnb.mean():.4f} +/- {recall_mnb.std():.4f}\")\n",
    "print(f\"Accuracy: {acc_mnb.mean():.4f} +/- {acc_mnb.std():.4f}\")\n",
    "print(f\"F1: {f1_mnb.mean():.4f} +/- {f1_mnb.std():.4f}\")\n",
    "print(f\"ROC AUC: {auc_mnb.mean():.4f} +/- {auc_mnb.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999efde",
   "metadata": {
    "id": "a999efde",
    "outputId": "265e72cd-45d9-4818-cb75-6470c6bf2b7d"
   },
   "outputs": [],
   "source": [
    "test['pure'] = pred_proba_pure\n",
    "test['intersect'] = pred_proba\n",
    "#test['mnb'] = pred_proba_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef95f24",
   "metadata": {
    "id": "7ef95f24",
    "outputId": "d9d5d01c-4dc6-4c0d-e370-a15d637ef360"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 3), dpi=500)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "kde('pure', 'Spam/Ham', test, ax=axs[0], title=\"Naive Bayes\")\n",
    "axs[0].set_ylim([0,25])\n",
    "kde('intersect', 'Spam/Ham', test, ax=axs[1], title=\"Intersect Bayes\")\n",
    "axs[1].set_ylim([0,25])\n",
    "#kde('mnb', 'Spam/Ham', test, ax=axs[2])\n",
    "fig.legend([\"$\\overline{spam}$\", \"$spam$\"])\n",
    "\n",
    "fig.text(0.5, 0.00, 'Probabilidade', ha='center')\n",
    "fig.text(0.08, 0.5, 'Frequência', va='center', rotation='vertical')\n",
    "fig.savefig(\"kde.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z01ZrSGBE4YH",
   "metadata": {
    "id": "z01ZrSGBE4YH"
   },
   "source": [
    "### testando novos dados. - IB com MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D1NZla5qFjQP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55168,
     "status": "ok",
     "timestamp": 1737519171003,
     "user": {
      "displayName": "Luyza Domingos",
      "userId": "00540737197809135596"
     },
     "user_tz": 180
    },
    "id": "D1NZla5qFjQP",
    "outputId": "a889f299-a6c1-4ae4-a87e-3e2cdc4db056"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# cria um dicionário de grupos\n",
    "def create_group_dict(train, text, target, solution_func):\n",
    "    group_dict = defaultdict(set)\n",
    "    for i, entry in enumerate(train[text]):\n",
    "        pairs = solution_func(entry)  # interseção de grupos de palavras\n",
    "        for group in pairs:\n",
    "            group_id = f\"{group['a']}_{group['b']}\"  # id unico\n",
    "            group_dict[group_id].add(i)\n",
    "    return group_dict\n",
    "\n",
    "# substituir palavras pelos ids dos grupos\n",
    "def replace_with_groups(entry, solution_func):\n",
    "    pairs = solution_func(entry)  # pares de palavras\n",
    "    transformed_text = []\n",
    "    for group in pairs:\n",
    "        group_id = f\"{group['a']}_{group['b']}\"  # id único\n",
    "        transformed_text.append(group_id)\n",
    "    return ' '.join(transformed_text)\n",
    "\n",
    "# calcular frequência das palavras\n",
    "def calculate_word_frequencies(data, text_column):\n",
    "    word_frequency = defaultdict(int) # dicionário para contar a frequência de palavras\n",
    "    for entry in data[text_column]:\n",
    "        for word in entry.split():\n",
    "            word_frequency[word] += 1\n",
    "    return word_frequency\n",
    "\n",
    "# classe para criar pares de palavras\n",
    "class Pairs:\n",
    "    def __init__(self, words_dict, words_dict_0, words_dict_1, word_frequency, min_threshold):\n",
    "        self.pairs_matrix = dict()\n",
    "        self.words_dict = words_dict\n",
    "        self.words_dict_1 = words_dict_1\n",
    "        self.words_dict_0 = words_dict_0\n",
    "        self.word_frequency = word_frequency\n",
    "        self.min_threshold = min_threshold\n",
    "\n",
    "    def get_pairs(self, entry):\n",
    "        pairs = []\n",
    "        words = list(set(entry.split(' ')))\n",
    "\n",
    "        # filtrar palavras acima da frequencia minima\n",
    "        words = [word for word in words if self.word_frequency[word] > self.min_threshold]\n",
    "\n",
    "        for j in range(len(words)):\n",
    "            for k in range(j+1, len(words)):\n",
    "                if words[j] not in self.words_dict or words[k] not in self.words_dict:\n",
    "                    continue\n",
    "                intersection = self.words_dict[words[j]] & self.words_dict[words[k]]\n",
    "                intersection_0 = self.words_dict_0.get(words[j], set()) & self.words_dict_0.get(words[k], set())\n",
    "                intersection_1 = self.words_dict_1.get(words[j], set()) & self.words_dict_1.get(words[k], set())\n",
    "\n",
    "                if len(intersection) > 0:\n",
    "                    pairs.append({\n",
    "                        'a': words[j],\n",
    "                        'b': words[k],\n",
    "                        'int': len(intersection),\n",
    "                        'int_0': len(intersection_0),\n",
    "                        'int_1': len(intersection_1)\n",
    "                    })\n",
    "        return pairs\n",
    "\n",
    "\n",
    "# heuristica gulosa\n",
    "def get_solution(pairs, entry):\n",
    "    remaining = set(entry.split(' '))\n",
    "    solution = []\n",
    "    if len(pairs) > 0:\n",
    "        for row in pairs:\n",
    "            if row['a'] in remaining and row['b'] in remaining:\n",
    "                solution.append(row)\n",
    "                remaining.remove(row['a'])\n",
    "                remaining.remove(row['b'])\n",
    "                if len(remaining) <= 1:\n",
    "                    break\n",
    "    for word in remaining:\n",
    "        solution.append({'a': word, 'b': word, 'int': 0, 'int_0': 0, 'int_1': 0})\n",
    "    return solution\n",
    "\n",
    "# aplicação do mnb\n",
    "def train_mnb(train, test, text, target, solution_func):\n",
    "\n",
    "    train[text] = train[text].apply(lambda x: replace_with_groups(x, solution_func))\n",
    "    test[text] = test[text].apply(lambda x: replace_with_groups(x, solution_func))\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train[text])\n",
    "    X_test = vectorizer.transform(test[text])\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, train[target])\n",
    "\n",
    "    predictions = clf.predict(X_test)\n",
    "    probabilities = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(test[target], predictions)\n",
    "    f1 = f1_score(test[target], predictions, average='weighted')\n",
    "    precision = precision_score(test[target], predictions, average='weighted')\n",
    "    recall = recall_score(test[target], predictions, average='weighted')\n",
    "    roc_auc = roc_auc_score(test[target], probabilities)\n",
    "\n",
    "    return accuracy, f1, precision, recall, roc_auc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    # 80/20 treino/teste\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=42, stratify=data[target])\n",
    "\n",
    "    # frequencia das palavras\n",
    "    word_frequency = calculate_word_frequencies(train, text)\n",
    "\n",
    "    # limiar de frequencia minima das palavras\n",
    "    min_threshold = 2\n",
    "\n",
    "    # criação dos dicionários de palavras\n",
    "    words_dict_1, words_dict_0, words_dict = defaultdict(set), defaultdict(set), defaultdict(set)\n",
    "    obj_pairs = Pairs(words_dict, words_dict_0, words_dict_1, word_frequency, min_threshold)\n",
    "\n",
    "    # cria a solução\n",
    "    def solution_func(entry):\n",
    "        pairs = obj_pairs.get_pairs(entry)\n",
    "        return get_solution(pairs, entry)  \n",
    "        # solution = get_solution(pairs, entry) \n",
    "        # print(f\"Grupos criados para o texto '{entry}': {solution}\")  # Opcional: para depuração\n",
    "        # return solution\n",
    "\n",
    "    accuracy, f1, precision, recall, roc_auc = train_mnb(train, test, text, target, solution_func)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Roc-Auc: {roc_auc}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
